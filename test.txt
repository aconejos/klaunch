===> User
uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
===> Configuring ...
===> Running preflight checks ... 
===> Check if Kafka is healthy ...
0 [main] DEBUG io.confluent.admin.utils.cli.KafkaReadyCommand  - Arguments Namespace(zookeeper_connect=null, min_expected_brokers=1, security_protocol=PLAINTEXT, config=null, bootstrap_servers=kafka1:19091,kafka2:19092,kafka3:19093, timeout=40000). 
63 [main] INFO org.apache.kafka.clients.admin.AdminClientConfig  - AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

102 [main] DEBUG org.apache.kafka.clients.admin.internals.AdminMetadataManager  - [AdminClient clientId=adminclient-1] Setting bootstrap cluster metadata Cluster(id = null, nodes = [kafka2:19092 (id: -2 rack: null), kafka1:19091 (id: -1 rack: null), kafka3:19093 (id: -3 rack: null)], partitions = [], controller = null).
442 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 7.1.1-ccs
443 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: 947fac5beb61836d
443 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1650899262499
445 [main] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Kafka admin client initialized
446 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Thread starting
471 [main] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Queueing Call(callName=listNodes, deadlineMs=1650899302533, tries=0, nextAllowedTryMs=0) with a timeout 30000 ms from now.
482 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka3 as 172.18.0.5
482 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka3:19093 (id: -3 rack: null) using address kafka3/172.18.0.5
520 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka3/172.18.0.5 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
524 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -3 disconnected.
527 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -3 (kafka3/172.18.0.5:19093) could not be established. Broker may not be available.
529 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka2 as 172.18.0.4
529 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka2:19092 (id: -2 rack: null) using address kafka2/172.18.0.4
532 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka2/172.18.0.4 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
532 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -2 disconnected.
532 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -2 (kafka2/172.18.0.4:19092) could not be established. Broker may not be available.
533 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka1 as 172.18.0.3
533 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka1:19091 (id: -1 rack: null) using address kafka1/172.18.0.3
535 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka1/172.18.0.3 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
536 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
536 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (kafka1/172.18.0.3:19091) could not be established. Broker may not be available.
639 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka3 as 172.18.0.5
639 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka3:19093 (id: -3 rack: null) using address kafka3/172.18.0.5
639 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka3/172.18.0.5 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
644 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -3 disconnected.
645 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -3 (kafka3/172.18.0.5:19093) could not be established. Broker may not be available.
645 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka2 as 172.18.0.4
645 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka2:19092 (id: -2 rack: null) using address kafka2/172.18.0.4
645 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka2/172.18.0.4 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
646 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -2 disconnected.
646 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -2 (kafka2/172.18.0.4:19092) could not be established. Broker may not be available.
646 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka1 as 172.18.0.3
646 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka1:19091 (id: -1 rack: null) using address kafka1/172.18.0.3
647 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka1/172.18.0.3 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
647 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
647 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (kafka1/172.18.0.3:19091) could not be established. Broker may not be available.
848 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka3 as 172.18.0.5
848 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka3:19093 (id: -3 rack: null) using address kafka3/172.18.0.5
849 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka3/172.18.0.5 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
850 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -3 disconnected.
850 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -3 (kafka3/172.18.0.5:19093) could not be established. Broker may not be available.
850 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka2 as 172.18.0.4
850 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka2:19092 (id: -2 rack: null) using address kafka2/172.18.0.4
851 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka2/172.18.0.4 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
851 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -2 disconnected.
851 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -2 (kafka2/172.18.0.4:19092) could not be established. Broker may not be available.
852 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka1 as 172.18.0.3
852 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka1:19091 (id: -1 rack: null) using address kafka1/172.18.0.3
852 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka1/172.18.0.3 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
853 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
853 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (kafka1/172.18.0.3:19091) could not be established. Broker may not be available.
1054 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka3 as 172.18.0.5
1054 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka3:19093 (id: -3 rack: null) using address kafka3/172.18.0.5
1056 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka3/172.18.0.5 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
1056 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -3 disconnected.
1056 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -3 (kafka3/172.18.0.5:19093) could not be established. Broker may not be available.
1157 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka2 as 172.18.0.4
1157 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka2:19092 (id: -2 rack: null) using address kafka2/172.18.0.4
1158 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka2/172.18.0.4 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
1158 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -2 disconnected.
1158 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -2 (kafka2/172.18.0.4:19092) could not be established. Broker may not be available.
1158 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka1 as 172.18.0.3
1158 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka1:19091 (id: -1 rack: null) using address kafka1/172.18.0.3
1159 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka1/172.18.0.3 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
1159 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
1159 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (kafka1/172.18.0.3:19091) could not be established. Broker may not be available.
1561 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka3 as 172.18.0.5
1562 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka3:19093 (id: -3 rack: null) using address kafka3/172.18.0.5
1562 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka3/172.18.0.5 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
1562 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -3 disconnected.
1562 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -3 (kafka3/172.18.0.5:19093) could not be established. Broker may not be available.
1663 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka2 as 172.18.0.4
1663 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka2:19092 (id: -2 rack: null) using address kafka2/172.18.0.4
1664 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka2/172.18.0.4 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
1664 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -2 disconnected.
1664 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -2 (kafka2/172.18.0.4:19092) could not be established. Broker may not be available.
1664 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka1 as 172.18.0.3
1664 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka1:19091 (id: -1 rack: null) using address kafka1/172.18.0.3
1665 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with kafka1/172.18.0.3 disconnected
java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
	at java.base/java.lang.Thread.run(Thread.java:829)
1665 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
1665 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (kafka1/172.18.0.3:19091) could not be established. Broker may not be available.
2247 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka3 as 172.18.0.5
2247 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka3:19093 (id: -3 rack: null) using address kafka3/172.18.0.5
2248 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -3
2521 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Completed connection to node -3. Fetching API versions.
2521 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating API versions fetch from node -3.
2551 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=0) and timeout 3600000 to node -3: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='7.1.1-ccs')
2720 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received API_VERSIONS response from node -3 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=0): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=8), ApiVersion(apiKey=1, minVersion=0, maxVersion=11), ApiVersion(apiKey=2, minVersion=0, maxVersion=5), ApiVersion(apiKey=3, minVersion=0, maxVersion=9), ApiVersion(apiKey=4, minVersion=0, maxVersion=4), ApiVersion(apiKey=5, minVersion=0, maxVersion=2), ApiVersion(apiKey=6, minVersion=0, maxVersion=6), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=7), ApiVersion(apiKey=10, minVersion=0, maxVersion=3), ApiVersion(apiKey=11, minVersion=0, maxVersion=7), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=4), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=3), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=5), ApiVersion(apiKey=20, minVersion=0, maxVersion=4), ApiVersion(apiKey=21, minVersion=0, maxVersion=1), ApiVersion(apiKey=22, minVersion=0, maxVersion=3), ApiVersion(apiKey=23, minVersion=0, maxVersion=3), ApiVersion(apiKey=24, minVersion=0, maxVersion=1), ApiVersion(apiKey=25, minVersion=0, maxVersion=1), ApiVersion(apiKey=26, minVersion=0, maxVersion=1), ApiVersion(apiKey=27, minVersion=0, maxVersion=0), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=2), ApiVersion(apiKey=30, minVersion=0, maxVersion=2), ApiVersion(apiKey=31, minVersion=0, maxVersion=2), ApiVersion(apiKey=32, minVersion=0, maxVersion=2), ApiVersion(apiKey=33, minVersion=0, maxVersion=1), ApiVersion(apiKey=34, minVersion=0, maxVersion=1), ApiVersion(apiKey=35, minVersion=0, maxVersion=1), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=2), ApiVersion(apiKey=38, minVersion=0, maxVersion=2), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=2), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=10000, minVersion=0, maxVersion=0), ApiVersion(apiKey=32766, minVersion=0, maxVersion=1), ApiVersion(apiKey=32767, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=-1, finalizedFeatures=[])
2769 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -3 has finalized features epoch: -1, finalized features: [], supported features: [], API versions: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 7 [usable: 7], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 7 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 3 [usable: 3], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 2 [usable: 2], CreateAcls(30): 0 to 2 [usable: 2], DeleteAcls(31): 0 to 2 [usable: 2], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 2 [usable: 2], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 2 [usable: 2], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): UNSUPPORTED, AlterClientQuotas(49): UNSUPPORTED, DescribeUserScramCredentials(50): UNSUPPORTED, AlterUserScramCredentials(51): UNSUPPORTED, AlterIsr(56): UNSUPPORTED, UpdateFeatures(57): UNSUPPORTED, DescribeCluster(60): UNSUPPORTED, DescribeProducers(61): UNSUPPORTED, DescribeTransactions(65): UNSUPPORTED, ListTransactions(66): UNSUPPORTED, AllocateProducerIds(67): UNSUPPORTED, UNKNOWN(10000): 0, UNKNOWN(32766): 0 to 1, UNKNOWN(32767): 0).
2769 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Sending MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to kafka3:19093 (id: -3 rack: null). correlationId=1, timeoutMs=27692
2770 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=9, clientId=adminclient-1, correlationId=1) and timeout 27692 to node -3: MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
2778 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received METADATA response from node -3 for request with header RequestHeader(apiKey=METADATA, apiVersion=9, clientId=adminclient-1, correlationId=1): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=2, host='kafka2', port=19092, rack='rack-0'), MetadataResponseBroker(nodeId=3, host='kafka3', port=19093, rack='rack-0'), MetadataResponseBroker(nodeId=1, host='kafka1', port=19091, rack='rack-0')], clusterId='rVFT0xMpRjSktFSh_qqZNw', controllerId=2, topics=[], clusterAuthorizedOperations=-2147483648)
2781 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.internals.AdminMetadataManager  - [AdminClient clientId=adminclient-1] Updating cluster metadata to Cluster(id = rVFT0xMpRjSktFSh_qqZNw, nodes = [kafka2:19092 (id: 2 rack: rack-0), kafka1:19091 (id: 1 rack: rack-0), kafka3:19093 (id: 3 rack: rack-0)], partitions = [], controller = kafka2:19092 (id: 2 rack: rack-0))
2782 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host kafka1 as 172.18.0.3
2782 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node kafka1:19091 (id: 1 rack: rack-0) using address kafka1/172.18.0.3
2784 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 1
2784 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Completed connection to node 1. Fetching API versions.
2784 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating API versions fetch from node 1.
2784 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=2) and timeout 3600000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='7.1.1-ccs')
2793 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=8), ApiVersion(apiKey=1, minVersion=0, maxVersion=11), ApiVersion(apiKey=2, minVersion=0, maxVersion=5), ApiVersion(apiKey=3, minVersion=0, maxVersion=9), ApiVersion(apiKey=4, minVersion=0, maxVersion=4), ApiVersion(apiKey=5, minVersion=0, maxVersion=2), ApiVersion(apiKey=6, minVersion=0, maxVersion=6), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=7), ApiVersion(apiKey=10, minVersion=0, maxVersion=3), ApiVersion(apiKey=11, minVersion=0, maxVersion=7), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=4), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=3), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=5), ApiVersion(apiKey=20, minVersion=0, maxVersion=4), ApiVersion(apiKey=21, minVersion=0, maxVersion=1), ApiVersion(apiKey=22, minVersion=0, maxVersion=3), ApiVersion(apiKey=23, minVersion=0, maxVersion=3), ApiVersion(apiKey=24, minVersion=0, maxVersion=1), ApiVersion(apiKey=25, minVersion=0, maxVersion=1), ApiVersion(apiKey=26, minVersion=0, maxVersion=1), ApiVersion(apiKey=27, minVersion=0, maxVersion=0), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=2), ApiVersion(apiKey=30, minVersion=0, maxVersion=2), ApiVersion(apiKey=31, minVersion=0, maxVersion=2), ApiVersion(apiKey=32, minVersion=0, maxVersion=2), ApiVersion(apiKey=33, minVersion=0, maxVersion=1), ApiVersion(apiKey=34, minVersion=0, maxVersion=1), ApiVersion(apiKey=35, minVersion=0, maxVersion=1), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=2), ApiVersion(apiKey=38, minVersion=0, maxVersion=2), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=2), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=10000, minVersion=0, maxVersion=0), ApiVersion(apiKey=32766, minVersion=0, maxVersion=1), ApiVersion(apiKey=32767, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=-1, finalizedFeatures=[])
2794 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node 1 has finalized features epoch: -1, finalized features: [], supported features: [], API versions: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 7 [usable: 7], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 7 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 3 [usable: 3], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 2 [usable: 2], CreateAcls(30): 0 to 2 [usable: 2], DeleteAcls(31): 0 to 2 [usable: 2], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 2 [usable: 2], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 2 [usable: 2], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): UNSUPPORTED, AlterClientQuotas(49): UNSUPPORTED, DescribeUserScramCredentials(50): UNSUPPORTED, AlterUserScramCredentials(51): UNSUPPORTED, AlterIsr(56): UNSUPPORTED, UpdateFeatures(57): UNSUPPORTED, DescribeCluster(60): UNSUPPORTED, DescribeProducers(61): UNSUPPORTED, DescribeTransactions(65): UNSUPPORTED, ListTransactions(66): UNSUPPORTED, AllocateProducerIds(67): UNSUPPORTED, UNKNOWN(10000): 0, UNKNOWN(32766): 0 to 1, UNKNOWN(32767): 0).
2794 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Sending DescribeClusterRequestData(includeClusterAuthorizedOperations=false) to kafka1:19091 (id: 1 rack: rack-0). correlationId=3, timeoutMs=29984
2794 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Version mismatch when attempting to send DescribeClusterRequestData(includeClusterAuthorizedOperations=false) with correlation id 3 to 1
org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support DESCRIBE_CLUSTER
2795 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Call(callName=listNodes, deadlineMs=1650899302533, tries=0, nextAllowedTryMs=0) attempting protocol downgrade and then retry.
2795 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Sending MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to kafka1:19091 (id: 1 rack: rack-0). correlationId=4, timeoutMs=30000
2795 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=9, clientId=adminclient-1, correlationId=4) and timeout 30000 to node 1: MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
2803 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received METADATA response from node 1 for request with header RequestHeader(apiKey=METADATA, apiVersion=9, clientId=adminclient-1, correlationId=4): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=2, host='kafka2', port=19092, rack='rack-0'), MetadataResponseBroker(nodeId=3, host='kafka3', port=19093, rack='rack-0'), MetadataResponseBroker(nodeId=1, host='kafka1', port=19091, rack='rack-0')], clusterId='rVFT0xMpRjSktFSh_qqZNw', controllerId=2, topics=[], clusterAuthorizedOperations=-2147483648)
2803 [main] DEBUG io.confluent.admin.utils.ClusterStatus  - Broker list: [kafka1:19091 (id: 1 rack: rack-0), kafka2:19092 (id: 2 rack: rack-0), kafka3:19093 (id: 3 rack: rack-0)]
===> Launching ... 
===> Launching kafka-connect ... 
[2022-04-25 15:07:46,783] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/var/log/kafka, -Dlog4j.configuration=file:/etc/kafka/connect-log4j.properties, -javaagent:/tmp/jmx_prometheus_javaagent-0.12.1.jar=8091:/tmp/kafka_connect.yml
	jvm.spec = Azul Systems, Inc., OpenJDK 64-Bit Server VM, 11.0.14.1, 11.0.14.1+1-LTS
	jvm.classpath = /etc/kafka-connect/jars/*:/usr/share/java/kafka/reflections-0.9.12.jar:/usr/share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/share/java/kafka/audience-annotations-0.5.0.jar:/usr/share/java/kafka/jakarta.inject-2.6.1.jar:/usr/share/java/kafka/connect-json-7.1.1-ccs.jar:/usr/share/java/kafka/netty-common-4.1.73.Final.jar:/usr/share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/share/java/kafka/jline-3.12.1.jar:/usr/share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/share/java/kafka/kafka-metadata-7.1.1-ccs.jar:/usr/share/java/kafka/jersey-container-servlet-2.34.jar:/usr/share/java/kafka/activation-1.1.1.jar:/usr/share/java/kafka/kafka-streams-scala_2.13-7.1.1-ccs.jar:/usr/share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/share/java/kafka/scala-library-2.13.6.jar:/usr/share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/share/java/kafka/hk2-api-2.6.1.jar:/usr/share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/share/java/kafka/maven-artifact-3.8.1.jar:/usr/share/java/kafka/metrics-core-2.2.0.jar:/usr/share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/share/java/kafka/kafka-clients-7.1.1-ccs.jar:/usr/share/java/kafka/lz4-java-1.8.0.jar:/usr/share/java/kafka/logredactor-metrics-1.0.8.jar:/usr/share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/share/java/kafka/argparse4j-0.7.0.jar:/usr/share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/share/java/kafka/jose4j-0.7.8.jar:/usr/share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/share/java/kafka/kafka-streams-7.1.1-ccs.jar:/usr/share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/share/java/kafka/kafka_2.13-7.1.1-ccs.jar:/usr/share/java/kafka/kafka-storage-api-7.1.1-ccs.jar:/usr/share/java/kafka/kafka-server-common-7.1.1-ccs.jar:/usr/share/java/kafka/commons-cli-1.4.jar:/usr/share/java/kafka/jackson-annotations-2.12.3.jar:/usr/share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/share/java/kafka/kafka.jar:/usr/share/java/kafka/jersey-common-2.34.jar:/usr/share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/share/java/kafka/connect-mirror-client-7.1.1-ccs.jar:/usr/share/java/kafka/jaxb-api-2.3.0.jar:/usr/share/java/kafka/kafka-shell-7.1.1-ccs.jar:/usr/share/java/kafka/kafka-storage-7.1.1-ccs.jar:/usr/share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/share/java/kafka/metrics-core-4.1.12.1.jar:/usr/share/java/kafka/zookeeper-3.6.3.jar:/usr/share/java/kafka/jopt-simple-5.0.4.jar:/usr/share/java/kafka/kafka-tools-7.1.1-ccs.jar:/usr/share/java/kafka/jersey-server-2.34.jar:/usr/share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/share/java/kafka/confluent-log4j-1.2.17-cp8.jar:/usr/share/java/kafka/javassist-3.27.0-GA.jar:/usr/share/java/kafka/commons-lang3-3.8.1.jar:/usr/share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/share/java/kafka/jackson-core-2.12.3.jar:/usr/share/java/kafka/kafka-streams-examples-7.1.1-ccs.jar:/usr/share/java/kafka/jackson-databind-2.12.3.jar:/usr/share/java/kafka/re2j-1.6.jar:/usr/share/java/kafka/connect-mirror-7.1.1-ccs.jar:/usr/share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/kafka/hk2-locator-2.6.1.jar:/usr/share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/share/java/kafka/trogdor-7.1.1-ccs.jar:/usr/share/java/kafka/kafka-streams-test-utils-7.1.1-ccs.jar:/usr/share/java/kafka/kafka-log4j-appender-7.1.1-ccs.jar:/usr/share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/share/java/kafka/kafka-raft-7.1.1-ccs.jar:/usr/share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/share/java/kafka/connect-basic-auth-extension-7.1.1-ccs.jar:/usr/share/java/kafka/jersey-hk2-2.34.jar:/usr/share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/share/java/kafka/paranamer-2.8.jar:/usr/share/java/kafka/plexus-utils-3.2.1.jar:/usr/share/java/kafka/connect-api-7.1.1-ccs.jar:/usr/share/java/kafka/connect-transforms-7.1.1-ccs.jar:/usr/share/java/kafka/slf4j-api-1.7.30.jar:/usr/share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/share/java/kafka/minimal-json-0.9.5.jar:/usr/share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/kafka/connect-runtime-7.1.1-ccs.jar:/usr/share/java/kafka/snappy-java-1.1.8.4.jar:/usr/share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/share/java/kafka/logredactor-1.0.8.jar:/usr/share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/share/java/kafka/jersey-client-2.34.jar:/usr/share/java/kafka/hk2-utils-2.6.1.jar:/usr/share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/share/java/kafka/scala-reflect-2.13.6.jar:/usr/share/java/confluent-common/common-metrics-7.1.1.jar:/usr/share/java/confluent-common/common-utils-7.1.1.jar:/usr/share/java/confluent-common/common-config-7.1.1.jar:/usr/share/java/confluent-common/build-tools-7.1.1.jar:/usr/share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/share/java/kafka-serde-tools/avro-1.11.0.jar:/usr/share/java/kafka-serde-tools/commons-validator-1.6.jar:/usr/share/java/kafka-serde-tools/wire-runtime-jvm-4.0.0.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-joda-2.12.3.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-provider-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-connect-json-schema-converter-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-converter-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-types-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-streams-json-schema-serde-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-connect-protobuf-converter-7.1.1.jar:/usr/share/java/kafka-serde-tools/joda-time-2.10.8.jar:/usr/share/java/kafka-serde-tools/org.everit.json.schema-1.12.2.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-data-7.1.1.jar:/usr/share/java/kafka-serde-tools/okio-jvm-3.0.0.jar:/usr/share/java/kafka-serde-tools/j2objc-annotations-1.3.jar:/usr/share/java/kafka-serde-tools/jsr305-3.0.2.jar:/usr/share/java/kafka-serde-tools/commons-compress-1.21.jar:/usr/share/java/kafka-serde-tools/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/kafka-serde-tools/re2j-1.3.jar:/usr/share/java/kafka-serde-tools/kafka-streams-7.1.1-ccs.jar:/usr/share/java/kafka-serde-tools/kafka-json-serializer-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-streams-avro-serde-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-streams-protobuf-serde-7.1.1.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-common-1.5.31.jar:/usr/share/java/kafka-serde-tools/jackson-annotations-2.12.3.jar:/usr/share/java/kafka-serde-tools/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/share/java/kafka-serde-tools/commons-logging-1.2.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-1.4.21.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-serializer-7.1.1.jar:/usr/share/java/kafka-serde-tools/proto-google-common-protos-2.5.1.jar:/usr/share/java/kafka-serde-tools/classgraph-4.8.21.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/error_prone_annotations-2.5.1.jar:/usr/share/java/kafka-serde-tools/kotlin-script-runtime-1.4.21.jar:/usr/share/java/kafka-serde-tools/validation-api-2.0.1.Final.jar:/usr/share/java/kafka-serde-tools/protobuf-java-3.17.3.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk8-1.5.31.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-provider-7.1.1.jar:/usr/share/java/kafka-serde-tools/commons-collections-3.2.2.jar:/usr/share/java/kafka-serde-tools/jackson-core-2.12.3.jar:/usr/share/java/kafka-serde-tools/protobuf-java-util-3.17.3.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jsr310-2.12.3.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-common-1.4.21.jar:/usr/share/java/kafka-serde-tools/handy-uri-templates-2.1.8.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk7-1.5.31.jar:/usr/share/java/kafka-serde-tools/checker-qual-3.8.0.jar:/usr/share/java/kafka-serde-tools/json-20201115.jar:/usr/share/java/kafka-serde-tools/jackson-databind-2.12.3.jar:/usr/share/java/kafka-serde-tools/commons-digester-1.8.1.jar:/usr/share/java/kafka-serde-tools/annotations-13.0.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-jvm-1.4.21.jar:/usr/share/java/kafka-serde-tools/kafka-schema-serializer-7.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-serializer-7.1.1.jar:/usr/share/java/kafka-serde-tools/swagger-annotations-2.1.10.jar:/usr/share/java/kafka-serde-tools/rocksdbjni-6.22.1.1.jar:/usr/share/java/kafka-serde-tools/gson-2.8.6.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-guava-2.12.3.jar:/usr/share/java/kafka-serde-tools/slf4j-api-1.7.30.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/kafka-serde-tools/failureaccess-1.0.1.jar:/usr/share/java/kafka-serde-tools/kotlinx-coroutines-core-1.3.7.jar:/usr/share/java/kafka-serde-tools/guava-30.1.1-jre.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/kafka-avro-serializer-7.1.1.jar:/usr/share/java/kafka-serde-tools/jackson-module-parameter-names-2.12.3.jar:/usr/share/java/kafka-serde-tools/wire-schema-jvm-4.0.0.jar:/usr/share/java/kafka-serde-tools/kafka-schema-registry-client-7.1.1.jar:/usr/share/java/kafka-serde-tools/scala-library-2.13.5.jar:/usr/share/java/monitoring-interceptors/monitoring-interceptors-7.1.1.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/connect-json-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/kafka-clients-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/logredactor-metrics-1.0.8.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-streams-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/kafka-shell-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-storage-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/kafka-tools-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp8.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/re2j-1.6.jar:/usr/bin/../share/java/kafka/connect-mirror-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/trogdor-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/bin/../share/java/kafka/kafka-raft-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/connect-api-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/connect-transforms-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/connect-runtime-7.1.1-ccs.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/logredactor-1.0.8.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/confluent-telemetry/confluent-metrics-7.1.1-ce.jar
	os.spec = Linux, amd64, 5.10.104-linuxkit
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo)
[2022-04-25 15:07:46,795] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed)
[2022-04-25 15:07:46,829] INFO Loading plugin from: /usr/share/java/cp-base-new (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,082] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/cp-base-new/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,083] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,083] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,083] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,083] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,083] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,084] INFO Loading plugin from: /usr/share/java/confluent-telemetry (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,740] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-telemetry/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:48,740] INFO Loading plugin from: /usr/share/java/rest-utils (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:49,642] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/rest-utils/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:49,643] INFO Loading plugin from: /usr/share/java/monitoring-interceptors (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:50,079] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/monitoring-interceptors/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:50,080] INFO Loading plugin from: /usr/share/java/acl (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/acl/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,958] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,959] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,960] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,961] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,962] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,962] INFO Added plugin 'io.confluent.kafka.secretregistry.client.config.provider.SecretConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,962] INFO Added plugin 'io.confluent.connect.security.ConnectSecurityExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:07:59,962] INFO Loading plugin from: /usr/share/java/schema-registry (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:02,312] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/schema-registry/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:02,320] INFO Loading plugin from: /usr/share/java/confluent-control-center (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,275] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-control-center/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,276] INFO Loading plugin from: /usr/share/java/confluent-common (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,283] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-common/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,284] INFO Loading plugin from: /usr/share/java/confluent-hub-client (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,435] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-hub-client/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,436] INFO Loading plugin from: /usr/share/java/kafka-serde-tools (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,773] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka-serde-tools/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:07,773] INFO Loading plugin from: /usr/share/java/kafka (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,786] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,786] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,786] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,786] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,787] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,791] INFO Loading plugin from: /usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,958] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,958] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:08,958] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,862] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@2c13da15 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,863] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'JsonSchemaConverter' and 'JsonSchema' to plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'ProtobufConverter' and 'Protobuf' to plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,864] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,865] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added alias 'ConnectSecurityExtension' to plugin 'io.confluent.connect.security.ConnectSecurityExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,866] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2022-04-25 15:08:10,922] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = docker-connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	group.id = connect-cluster-group
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = docker-connect-offsets
	plugin.path = [/usr/share/java, /usr/share/confluent-hub-components]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = connect
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = docker-connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig)
[2022-04-25 15:08:10,924] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:10,926] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,986] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:10,987] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:10,987] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:10,987] INFO Kafka startTimeMs: 1650899290986 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,288] INFO Kafka cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,289] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,295] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,295] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,295] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,304] INFO Logging initialized @25958ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
[2022-04-25 15:08:11,351] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,352] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,360] INFO jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.14.1+1-LTS (org.eclipse.jetty.server.Server)
[2022-04-25 15:08:11,385] INFO Started http_8083@574985d8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector)
[2022-04-25 15:08:11,385] INFO Started @26039ms (org.eclipse.jetty.server.Server)
[2022-04-25 15:08:11,408] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,408] INFO REST server listening at http://172.18.0.6:8083/, advertising URL http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,408] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,408] INFO REST admin endpoints at http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,408] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,411] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,412] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,415] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,416] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,416] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,416] INFO Kafka startTimeMs: 1650899291416 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,431] INFO Kafka cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,432] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,435] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,435] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,435] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,439] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy)
[2022-04-25 15:08:11,446] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,447] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,449] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,450] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,450] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,450] INFO Kafka startTimeMs: 1650899291450 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,463] INFO Kafka cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,463] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,465] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,465] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,465] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,468] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,468] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,468] INFO Kafka startTimeMs: 1650899291468 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,596] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:08:11,597] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:08:11,597] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,598] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,600] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,601] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,601] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,601] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,601] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,601] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,601] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,601] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,601] INFO Kafka startTimeMs: 1650899291601 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,611] INFO Kafka cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,612] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,614] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,614] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,614] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,621] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,621] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,625] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,625] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,625] INFO Kafka startTimeMs: 1650899291625 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,634] INFO Kafka cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,634] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,636] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,636] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,636] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,640] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,640] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,642] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,642] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,642] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,643] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,643] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,643] INFO Kafka startTimeMs: 1650899291643 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,652] INFO Kafka cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,653] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,654] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,655] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,655] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,667] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,667] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,669] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,670] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,670] INFO Kafka startTimeMs: 1650899291669 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,680] INFO Kafka cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.connect.util.ConnectUtils)
[2022-04-25 15:08:11,680] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,685] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,685] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,685] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:08:11,704] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,704] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,704] INFO Kafka startTimeMs: 1650899291704 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,708] INFO Kafka Connect distributed worker initialization took 24912ms (org.apache.kafka.connect.cli.ConnectDistributed)
[2022-04-25 15:08:11,708] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect)
[2022-04-25 15:08:11,709] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,709] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:08:11,709] INFO Worker starting (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:08:11,709] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
[2022-04-25 15:08:11,709] INFO Starting KafkaBasedLog with topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:11,710] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2022-04-25 15:08:11,712] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,712] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,712] INFO Kafka startTimeMs: 1650899291712 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:11,751] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:11,864] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
[2022-04-25 15:08:11,864] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
[2022-04-25 15:08:11,865] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
[2022-04-25 15:08:12,067] INFO Created topic (name=docker-connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka1:19091,kafka2:19092,kafka3:19093 (org.apache.kafka.connect.util.TopicAdmin)
[2022-04-25 15:08:12,081] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,108] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,108] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,108] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,108] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,113] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,113] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,113] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,113] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,114] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,114] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,114] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,114] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,114] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,114] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,114] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,115] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,115] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,115] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,115] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,115] INFO Kafka startTimeMs: 1650899292115 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,169] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,183] INFO [Producer clientId=producer-1] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:08:12,253] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,254] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,255] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,255] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,255] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,255] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,255] INFO Kafka startTimeMs: 1650899292255 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,272] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:08:12,280] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Subscribed to partition(s): docker-connect-offsets-0, docker-connect-offsets-5, docker-connect-offsets-10, docker-connect-offsets-20, docker-connect-offsets-15, docker-connect-offsets-9, docker-connect-offsets-11, docker-connect-offsets-16, docker-connect-offsets-4, docker-connect-offsets-17, docker-connect-offsets-3, docker-connect-offsets-24, docker-connect-offsets-23, docker-connect-offsets-13, docker-connect-offsets-18, docker-connect-offsets-22, docker-connect-offsets-2, docker-connect-offsets-8, docker-connect-offsets-12, docker-connect-offsets-19, docker-connect-offsets-14, docker-connect-offsets-1, docker-connect-offsets-6, docker-connect-offsets-7, docker-connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2022-04-25 15:08:12,285] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,285] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,286] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,598] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,600] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,600] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,600] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,600] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,600] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,600] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,600] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,641] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,642] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,642] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,642] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,643] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,643] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,643] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,644] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,662] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,662] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,663] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,663] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,663] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,664] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,664] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,665] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,665] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Resetting offset for partition docker-connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,669] INFO Finished reading KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:12,670] INFO Started KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:12,670] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
[2022-04-25 15:08:12,687] INFO Worker started (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:08:12,687] INFO Starting KafkaBasedLog with topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:12,915] INFO Created topic (name=docker-connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka1:19091,kafka2:19092,kafka3:19093 (org.apache.kafka.connect.util.TopicAdmin)
[2022-04-25 15:08:12,916] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:12,925] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,926] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,926] INFO Kafka startTimeMs: 1650899292925 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,927] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,944] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:12,945] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,945] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,945] INFO Kafka startTimeMs: 1650899292945 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:12,947] INFO [Producer clientId=producer-2] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:08:12,955] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:08:12,961] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Subscribed to partition(s): docker-connect-status-0, docker-connect-status-4, docker-connect-status-1, docker-connect-status-2, docker-connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2022-04-25 15:08:12,961] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,961] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,962] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,962] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:12,962] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,008] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Resetting offset for partition docker-connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka1:19091 (id: 1 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,026] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Resetting offset for partition docker-connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,027] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Resetting offset for partition docker-connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka3:19093 (id: 3 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,028] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Resetting offset for partition docker-connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,028] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Resetting offset for partition docker-connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,029] INFO Finished reading KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:13,029] INFO Started KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:13,044] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 15:08:13,044] INFO Starting KafkaBasedLog with topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:13,142] INFO Created topic (name=docker-connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka1:19091,kafka2:19092,kafka3:19093 (org.apache.kafka.connect.util.TopicAdmin)
[2022-04-25 15:08:13,147] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,150] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,151] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,151] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,151] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,151] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,151] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,156] INFO Started o.e.j.s.ServletContextHandler@697b105e{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[2022-04-25 15:08:13,161] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer)
[2022-04-25 15:08:13,162] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect)
[2022-04-25 15:08:13,158] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,163] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,163] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,163] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,163] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,164] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,164] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,164] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,164] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,164] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,164] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:08:13,164] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:13,164] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:13,164] INFO Kafka startTimeMs: 1650899293164 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:13,165] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,170] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,170] WARN The configuration 'auto.create.topics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,170] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,170] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,170] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,170] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,170] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2022-04-25 15:08:13,171] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:13,171] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:13,171] INFO Kafka startTimeMs: 1650899293171 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:08:13,177] INFO [Producer clientId=producer-3] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:08:13,178] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:08:13,179] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Subscribed to partition(s): docker-connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2022-04-25 15:08:13,179] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Seeking to EARLIEST offset of partition docker-connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,194] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Resetting offset for partition docker-connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka2:19092 (id: 2 rack: rack-0)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2022-04-25 15:08:13,195] INFO Finished reading KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:13,195] INFO Started KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
[2022-04-25 15:08:13,195] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 15:08:13,195] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:08:13,207] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:08:14,490] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:08:14,492] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:08:14,492] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:08:14,560] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:08:14,605] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:08:15,152] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:08:15,153] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:08:15,153] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:08:15,153] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:08:15,239] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:13:11,603] INFO [AdminClient clientId=adminclient-8] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:17:11,871] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:17:12,179] INFO [Producer clientId=producer-1] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:17:12,583] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:17:12,875] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:17:12,890] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:17:12,977] INFO [Producer clientId=producer-2] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:17:13,217] INFO [Producer clientId=producer-3] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:18:11,498] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:18:11,499] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:20:04,075] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper)
java.lang.NullPointerException
	at org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.createConnector(ConnectorsResource.java:159)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:392)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:20:13,429] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper)
java.lang.NullPointerException
	at org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.createConnector(ConnectorsResource.java:159)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:392)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:20:43,668] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper)
java.lang.NullPointerException
	at org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.createConnector(ConnectorsResource.java:159)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:392)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:22:13,201] INFO [Producer clientId=producer-3] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:23:11,379] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:24:22,957] INFO Cluster created with settings {hosts=[host.docker.internal:27017, host.docker.internal:27018, host.docker.internal:27019], mode=MULTIPLE, requiredClusterType=REPLICA_SET, serverSelectionTimeout='30000 ms', requiredReplicaSetName='replset'} (org.mongodb.driver.cluster)
[2022-04-25 15:24:22,957] INFO Adding discovered server host.docker.internal:27017 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:24:22,984] INFO Adding discovered server host.docker.internal:27018 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:24:22,985] INFO Adding discovered server host.docker.internal:27019 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,034] INFO Opened connection [connectionId{localValue:4, serverValue:68}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,041] INFO Opened connection [connectionId{localValue:2, serverValue:72}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,034] INFO Opened connection [connectionId{localValue:5, serverValue:67}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,034] INFO Opened connection [connectionId{localValue:1, serverValue:73}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,034] INFO Opened connection [connectionId{localValue:3, serverValue:69}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,042] INFO Opened connection [connectionId{localValue:6, serverValue:66}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,041] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19234446, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c77f4d883b279f4bb6, counter=7}, lastWriteDate=Mon Apr 25 15:24:18 GMT 2022, lastUpdateTimeNanos=1705013038284} (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,041] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27019, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19683755, setName='replset', canonicalAddress=host.docker.internal:27019, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6ca7cdfe7fb5219eeb2, counter=5}, lastWriteDate=Mon Apr 25 15:24:18 GMT 2022, lastUpdateTimeNanos=1705014069113} (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,042] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27018, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19244749, setName='replset', canonicalAddress=host.docker.internal:27018, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c8df22631862829130, counter=5}, lastWriteDate=Mon Apr 25 15:24:18 GMT 2022, lastUpdateTimeNanos=1705013060240} (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,044] INFO Setting max election id to 7fffffff0000000000000001 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,044] INFO Setting max set version to 2 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,044] INFO Discovered replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,061] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 15:24:23,086] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,102] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,102] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,113] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=2, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,150] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=2, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,151] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=2, connectorIds=[mongoDB-sourcetest], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,151] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,153] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,161] INFO Creating connector mongoDB-sourcetest of type com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,162] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:24:23,162] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:24:23,168] INFO Instantiated connector mongoDB-sourcetest with version 1.6.1-dirty of type class com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,168] INFO Finished creating connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,169] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,184] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:24:23,185] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:24:23,223] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Tasks [mongoDB-sourcetest-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,237] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,238] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,238] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,240] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=3, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,250] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=3, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:24:23,254] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=4, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,255] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,256] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,257] INFO Creating task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,258] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2022-04-25 15:24:23,258] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:24:23,260] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.source.MongoSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2022-04-25 15:24:23,260] INFO Instantiated task mongoDB-sourcetest-0 with version 1.6.1-dirty of type com.mongodb.kafka.connect.source.MongoSourceTask (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,261] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:24:23,262] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,262] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:24:23,262] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,262] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,267] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:24:23,267] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:24:23,271] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:24:23,274] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongoDB-sourcetest-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:24:23,280] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:24:23,280] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:24:23,281] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:24:23,281] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:24:23,281] INFO Kafka startTimeMs: 1650900263280 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:24:23,296] INFO Starting MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:24:23,298] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:24:23,301] INFO Cluster created with settings {hosts=[host.docker.internal:27017, host.docker.internal:27018, host.docker.internal:27019], mode=MULTIPLE, requiredClusterType=REPLICA_SET, serverSelectionTimeout='30000 ms', requiredReplicaSetName='replset'} (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,302] INFO Adding discovered server host.docker.internal:27017 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,303] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:24:23,314] INFO Adding discovered server host.docker.internal:27018 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,315] INFO Adding discovered server host.docker.internal:27019 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,319] INFO Opened connection [connectionId{localValue:8, serverValue:75}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,320] INFO Opened connection [connectionId{localValue:7, serverValue:74}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,321] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5008510, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c77f4d883b279f4bb6, counter=7}, lastWriteDate=Mon Apr 25 15:24:18 GMT 2022, lastUpdateTimeNanos=1705300623371} (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,322] INFO Setting max election id to 7fffffff0000000000000001 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,322] INFO Setting max set version to 2 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,322] INFO Discovered replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,323] INFO Opened connection [connectionId{localValue:10, serverValue:69}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,325] INFO Opened connection [connectionId{localValue:9, serverValue:68}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,328] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27018, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=7820679, setName='replset', canonicalAddress=host.docker.internal:27018, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c8df22631862829130, counter=5}, lastWriteDate=Mon Apr 25 15:24:18 GMT 2022, lastUpdateTimeNanos=1705304939231} (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,333] INFO Opened connection [connectionId{localValue:11, serverValue:71}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,333] INFO Opened connection [connectionId{localValue:12, serverValue:70}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,334] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27019, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2244007, setName='replset', canonicalAddress=host.docker.internal:27019, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6ca7cdfe7fb5219eeb2, counter=5}, lastWriteDate=Mon Apr 25 15:24:18 GMT 2022, lastUpdateTimeNanos=1705312950010} (org.mongodb.driver.cluster)
[2022-04-25 15:24:23,356] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:24:23,389] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:24:23,413] INFO Opened connection [connectionId{localValue:13, serverValue:76}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:24:23,432] INFO Started MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:24:23,432] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:24:23,433] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:24:33,299] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:24:43,301] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:24:53,281] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:03,283] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:13,284] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:23,263] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:33,264] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:43,265] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:53,244] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:56,598] INFO Successfully processed removal of connector 'mongoDB-sourcetest' (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 15:25:56,600] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:56,604] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling connector-only config update by stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:56,604] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:25:56,604] INFO Scheduled shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 15:25:56,604] INFO Completed shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 15:25:56,608] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:56,608] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:56,611] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=4, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:56,620] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=4, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:56,621] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:25:56,621] WARN Ignoring stop request for unowned connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:25:56,621] WARN Ignoring await stop request for non-present connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:25:56,622] INFO Stopping task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:25:59,444] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:25:59,444] INFO Stopping MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:25:59,456] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 15:25:59,459] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:25:59,459] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:25:59,459] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:25:59,459] INFO App info kafka.producer for connector-producer-mongoDB-sourcetest-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:25:59,463] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:59,465] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:59,465] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=6, connectorIds=[], taskIds=[], revokedConnectorIds=[mongoDB-sourcetest], revokedTaskIds=[mongoDB-sourcetest-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:59,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 6 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:59,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:59,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:59,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:59,471] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=5, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:59,478] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=5, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:25:59,478] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=6, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:59,478] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 6 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:25:59,478] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,119] INFO Cluster created with settings {hosts=[host.docker.internal:27017, host.docker.internal:27018, host.docker.internal:27019], mode=MULTIPLE, requiredClusterType=REPLICA_SET, serverSelectionTimeout='30000 ms', requiredReplicaSetName='replset'} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,119] INFO Adding discovered server host.docker.internal:27017 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,120] INFO Adding discovered server host.docker.internal:27018 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,121] INFO Adding discovered server host.docker.internal:27019 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,122] INFO Opened connection [connectionId{localValue:14, serverValue:78}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,122] INFO Opened connection [connectionId{localValue:15, serverValue:77}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,122] INFO Opened connection [connectionId{localValue:16, serverValue:70}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,123] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1074863, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c77f4d883b279f4bb6, counter=7}, lastWriteDate=Mon Apr 25 15:26:18 GMT 2022, lastUpdateTimeNanos=1824191149977} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,126] INFO Setting max election id to 7fffffff0000000000000001 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,126] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27018, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=800075, setName='replset', canonicalAddress=host.docker.internal:27018, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c8df22631862829130, counter=5}, lastWriteDate=Mon Apr 25 15:26:18 GMT 2022, lastUpdateTimeNanos=1824191321331} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,126] INFO Opened connection [connectionId{localValue:19, serverValue:73}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,126] INFO Setting max set version to 2 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,127] INFO Opened connection [connectionId{localValue:18, serverValue:72}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,127] INFO Opened connection [connectionId{localValue:17, serverValue:71}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,127] INFO Discovered replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,127] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27019, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4654179, setName='replset', canonicalAddress=host.docker.internal:27019, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6ca7cdfe7fb5219eeb2, counter=5}, lastWriteDate=Mon Apr 25 15:26:18 GMT 2022, lastUpdateTimeNanos=1824195975759} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,131] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 15:26:22,135] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,137] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,137] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,138] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=6, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,145] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=6, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,145] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=7, connectorIds=[mongoDB-sourcetest], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,145] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,146] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,146] INFO Creating connector mongoDB-sourcetest of type com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,146] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:26:22,146] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:26:22,146] INFO Instantiated connector mongoDB-sourcetest with version 1.6.1-dirty of type class com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,147] INFO Finished creating connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,147] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,148] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:26:22,148] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:26:22,157] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Tasks [mongoDB-sourcetest-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,161] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,161] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,161] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,162] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=7, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,167] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=7, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:26:22,167] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=9, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,168] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 9 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,168] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,169] INFO Creating task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,169] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2022-04-25 15:26:22,169] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:26:22,169] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.source.MongoSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2022-04-25 15:26:22,169] INFO Instantiated task mongoDB-sourcetest-0 with version 1.6.1-dirty of type com.mongodb.kafka.connect.source.MongoSourceTask (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,170] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:26:22,170] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,170] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:26:22,170] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,170] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,171] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:26:22,171] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:26:22,172] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:26:22,172] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongoDB-sourcetest-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:26:22,175] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:26:22,175] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:26:22,176] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:26:22,176] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:26:22,176] INFO Kafka startTimeMs: 1650900382176 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:26:22,177] INFO Starting MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:26:22,177] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:26:22,179] INFO Cluster created with settings {hosts=[host.docker.internal:27017, host.docker.internal:27018, host.docker.internal:27019], mode=MULTIPLE, requiredClusterType=REPLICA_SET, serverSelectionTimeout='30000 ms', requiredReplicaSetName='replset'} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,179] INFO Adding discovered server host.docker.internal:27017 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,179] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:26:22,180] INFO Adding discovered server host.docker.internal:27018 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,180] INFO Adding discovered server host.docker.internal:27019 to client view of cluster (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,186] INFO Opened connection [connectionId{localValue:23, serverValue:73}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,186] INFO Opened connection [connectionId{localValue:22, serverValue:72}] to host.docker.internal:27018 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,186] INFO Opened connection [connectionId{localValue:20, serverValue:79}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,186] INFO Opened connection [connectionId{localValue:21, serverValue:80}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,186] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27018, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4593127, setName='replset', canonicalAddress=host.docker.internal:27018, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c8df22631862829130, counter=5}, lastWriteDate=Mon Apr 25 15:26:18 GMT 2022, lastUpdateTimeNanos=1824254995056} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,186] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5443962, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c77f4d883b279f4bb6, counter=7}, lastWriteDate=Mon Apr 25 15:26:18 GMT 2022, lastUpdateTimeNanos=1824255641113} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,187] INFO Opened connection [connectionId{localValue:24, serverValue:74}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,187] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27019, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4158213, setName='replset', canonicalAddress=host.docker.internal:27019, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6ca7cdfe7fb5219eeb2, counter=5}, lastWriteDate=Mon Apr 25 15:26:18 GMT 2022, lastUpdateTimeNanos=1824255879525} (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,188] INFO Setting max election id to 7fffffff0000000000000001 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,189] INFO Setting max set version to 2 from replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,189] INFO Discovered replica set primary host.docker.internal:27017 (org.mongodb.driver.cluster)
[2022-04-25 15:26:22,190] INFO Opened connection [connectionId{localValue:25, serverValue:75}] to host.docker.internal:27019 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,204] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:26:22,205] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:26:22,208] INFO Opened connection [connectionId{localValue:26, serverValue:81}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:26:22,209] INFO Started MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:26:22,209] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:26:22,210] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:26:32,178] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:26:42,179] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:26:52,157] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:27:02,161] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:27:12,161] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:27:22,143] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:27:23,070] WARN [Producer clientId=connector-producer-mongoDB-sourcetest-0] Error while fetching metadata with correlation id 3 : {mongoDBsource.testdb.newcol=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:27:32,144] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:27:42,163] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:27:52,143] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:28:02,143] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:28:12,144] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:28:22,124] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:28:32,126] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:28:42,126] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:28:52,106] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:29:02,107] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:29:12,108] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:29:22,089] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:29:32,091] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:29:42,091] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:29:52,071] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:30:02,074] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:30:12,104] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:30:22,084] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:30:32,084] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:30:36,797] INFO Exception in monitor thread while connecting to server host.docker.internal:27019 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27019. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6ca7cdfe7fb5219eeb2"}, "counter": 6}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 14997, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900628, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900628, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:30:42,085] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:30:46,782] INFO Exception in monitor thread while connecting to server host.docker.internal:27019 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27019. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6ca7cdfe7fb5219eeb2"}, "counter": 6}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 4996, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900638, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900638, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:30:52,064] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:31:02,065] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:31:06,797] INFO Exception in monitor thread while connecting to server host.docker.internal:27019 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 10 more
[2022-04-25 15:31:12,066] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:31:22,045] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:31:27,976] INFO Exception in monitor thread while connecting to server host.docker.internal:27018 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27018. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c8df22631862829130"}, "counter": 6}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 14999, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900678, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900678, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:31:32,046] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:31:37,981] INFO Exception in monitor thread while connecting to server host.docker.internal:27018 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27018. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c8df22631862829130"}, "counter": 6}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 4999, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900688, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900688, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:31:42,048] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:31:52,028] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:31:52,319] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1743650, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='null', tagSet=TagSet{[]}, electionId=null, setVersion=2, topologyVersion=TopologyVersion{processId=6266b6c77f4d883b279f4bb6, counter=8}, lastWriteDate=Mon Apr 25 15:31:48 GMT 2022, lastUpdateTimeNanos=2154621532401} (org.mongodb.driver.cluster)
[2022-04-25 15:31:57,975] INFO Exception in monitor thread while connecting to server host.docker.internal:27018 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 10 more
[2022-04-25 15:32:02,029] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:32:12,031] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:32:22,011] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:32:32,012] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:32:34,693] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 15000, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:35,239] INFO An exception occurred when trying to get the next item from the Change Stream (com.mongodb.kafka.connect.source.MongoSourceTask)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 14459, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.open(UsageTrackingInternalConnection.java:53)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.open(DefaultConnectionPool.java:495)
	at com.mongodb.internal.connection.DefaultConnectionPool$OpenConcurrencyLimiter.openOrGetAvailable(DefaultConnectionPool.java:855)
	at com.mongodb.internal.connection.DefaultConnectionPool$OpenConcurrencyLimiter.openOrGetAvailable(DefaultConnectionPool.java:805)
	at com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:154)
	at com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:144)
	at com.mongodb.internal.connection.DefaultServer.getConnection(DefaultServer.java:92)
	at com.mongodb.internal.binding.ClusterBinding$ClusterBindingConnectionSource.getConnection(ClusterBinding.java:141)
	at com.mongodb.client.internal.ClientSessionBinding$SessionBindingConnectionSource.getConnection(ClientSessionBinding.java:163)
	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.connection(QueryBatchCursor.java:528)
	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:510)
	at com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)
	at com.mongodb.internal.operation.QueryBatchCursor.tryHasNext(QueryBatchCursor.java:223)
	at com.mongodb.internal.operation.QueryBatchCursor.lambda$tryNext$0(QueryBatchCursor.java:206)
	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)
	at com.mongodb.internal.operation.QueryBatchCursor.tryNext(QueryBatchCursor.java:205)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:102)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:98)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor.resumeableOperation(ChangeStreamBatchCursor.java:195)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor.tryNext(ChangeStreamBatchCursor.java:98)
	at com.mongodb.client.internal.MongoChangeStreamCursorImpl.tryNext(MongoChangeStreamCursorImpl.java:78)
	at com.mongodb.kafka.connect.source.MongoSourceTask.getNextDocument(MongoSourceTask.java:620)
	at com.mongodb.kafka.connect.source.MongoSourceTask.poll(MongoSourceTask.java:223)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:304)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:248)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:40,234] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:32:40,295] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:32:40,297] INFO Resuming the change stream after the previous offset: {"_data": "826266BE68000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266BE68227BA85DA75410DD0004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:32:40,299] INFO No server chosen by com.mongodb.client.internal.MongoClientDelegate$1@2594ac19 from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=host.docker.internal:27019, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message}, caused by {java.net.SocketTimeoutException: Read timed out}}, ServerDescription{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 15000, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}}}, ServerDescription{address=host.docker.internal:27018, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message}, caused by {java.net.SocketTimeoutException: Read timed out}}]}. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster)
[2022-04-25 15:32:40,301] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 9395, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:40,806] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 8891, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:41,313] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 8385, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:41,817] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 7880, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:42,015] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:32:42,319] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 7379, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:42,822] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 6876, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:43,305] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 6372, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:43,808] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 5870, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:44,312] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 5366, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:44,814] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 4864, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:45,315] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 4363, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:45,818] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 3861, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:46,322] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 3357, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:46,824] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 2855, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:47,327] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 2353, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:47,831] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 1850, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:48,332] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 1348, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:48,835] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 846, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:49,337] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 91 (ShutdownInProgress): 'The server is in quiesce mode and will shut down' on server host.docker.internal:27017. The full response is {"topologyVersion": {"processId": {"$oid": "6266b6c77f4d883b279f4bb6"}, "counter": 9}, "ok": 0.0, "errmsg": "The server is in quiesce mode and will shut down", "code": 91, "codeName": "ShutdownInProgress", "remainingQuiesceTimeMillis": 344, "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650900708, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650900708, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:50,754] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:32:51,995] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:33:01,265] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 10 more
[2022-04-25 15:33:01,996] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:33:10,281] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:33:10,281] ERROR WorkerSourceTask{id=mongoDB-sourcetest-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask)
com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting for a server that matches com.mongodb.client.internal.MongoClientDelegate$1@2594ac19. Client view of cluster state is {type=REPLICA_SET, servers=[{address=host.docker.internal:27019, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message}, caused by {java.net.SocketTimeoutException: Read timed out}}, {address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message}, caused by {java.net.SocketTimeoutException: Read timed out}}, {address=host.docker.internal:27018, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message}, caused by {java.net.SocketTimeoutException: Read timed out}}]
	at com.mongodb.internal.connection.BaseCluster.createTimeoutException(BaseCluster.java:405)
	at com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:118)
	at com.mongodb.internal.connection.AbstractMultiServerCluster.selectServer(AbstractMultiServerCluster.java:50)
	at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:139)
	at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:94)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:276)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:176)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.execute(ChangeStreamIterableImpl.java:204)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.access$000(ChangeStreamIterableImpl.java:53)
	at com.mongodb.client.internal.ChangeStreamIterableImpl$1.cursor(ChangeStreamIterableImpl.java:129)
	at com.mongodb.client.internal.ChangeStreamIterableImpl$1.cursor(ChangeStreamIterableImpl.java:121)
	at com.mongodb.kafka.connect.source.MongoSourceTask.tryCreateCursor(MongoSourceTask.java:426)
	at com.mongodb.kafka.connect.source.MongoSourceTask.createCursor(MongoSourceTask.java:382)
	at com.mongodb.kafka.connect.source.MongoSourceTask.initializeCursorAndHeartbeatManager(MongoSourceTask.java:369)
	at com.mongodb.kafka.connect.source.MongoSourceTask.getNextDocument(MongoSourceTask.java:615)
	at com.mongodb.kafka.connect.source.MongoSourceTask.poll(MongoSourceTask.java:223)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:304)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:248)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:33:10,281] INFO Stopping MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:33:10,282] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 15:33:10,285] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:33:10,285] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:33:10,285] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:33:10,285] INFO App info kafka.producer for connector-producer-mongoDB-sourcetest-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:33:11,997] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:33:21,978] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:33:31,979] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:33:41,980] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:33:43,246] INFO [Producer clientId=producer-3] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:33:51,960] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:34:01,961] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:34:11,964] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:34:21,943] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:34:31,944] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:34:41,945] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:34:51,926] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:35:01,927] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:35:11,928] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:35:21,908] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:35:31,910] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:35:41,911] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:35:51,891] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:36:01,892] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:36:11,892] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:36:21,873] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:36:31,874] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:36:40,266] INFO [Producer clientId=producer-2] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:36:41,875] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:36:51,854] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:37:01,855] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:37:11,855] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:37:21,835] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:37:31,836] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:37:40,098] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:37:40,099] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:37:41,837] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:37:51,816] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:38:01,817] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:38:11,818] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:38:21,798] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:38:31,798] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:38:41,799] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:38:51,779] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:39:01,780] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:39:11,781] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:39:21,761] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:39:31,761] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:39:41,762] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:39:51,742] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:40:01,742] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:40:11,743] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:40:21,722] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:40:31,723] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:40:41,723] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:40:51,703] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:41:01,704] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:41:11,704] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:41:21,684] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:41:31,685] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:41:41,686] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:41:51,665] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:42:01,666] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:42:11,666] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:42:21,647] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:42:31,648] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:42:39,972] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:42:41,649] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:42:51,628] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:43:01,629] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:43:11,630] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:43:21,610] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:43:31,610] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:43:41,611] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:43:51,591] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:44:01,592] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:44:11,592] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:44:21,499] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:44:31,500] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:44:41,501] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:44:51,481] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:45:01,481] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:45:11,482] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:45:21,461] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:45:31,462] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:45:41,464] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:45:51,443] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:46:01,444] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:46:11,445] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:46:21,425] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:46:31,425] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:46:41,427] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:46:51,407] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:47:01,409] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:47:11,412] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:47:21,394] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:47:31,395] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:47:39,788] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:47:41,397] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:47:50,927] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 15:47:51,376] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:48:01,377] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:48:11,379] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:48:21,359] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:48:31,360] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:48:41,360] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:48:50,852] INFO Successfully processed removal of connector 'mongoDB-sourcetest' (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 15:48:50,852] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,854] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling connector-only config update by stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,854] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:48:50,854] INFO Scheduled shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 15:48:50,858] INFO Completed shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 15:48:50,859] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,859] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,864] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=8, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,873] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=8, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,874] INFO Stopping task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:48:50,874] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:48:50,874] WARN Ignoring stop request for unowned connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:48:50,875] WARN Ignoring await stop request for non-present connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:48:50,876] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,892] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,892] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=11, connectorIds=[], taskIds=[], revokedConnectorIds=[mongoDB-sourcetest], revokedTaskIds=[mongoDB-sourcetest-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,893] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 11 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,893] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,893] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,893] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,895] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=9, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,925] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=9, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:48:50,925] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=11, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,925] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 11 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:50,925] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:48:56,981] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 15:49:25,930] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 15:49:25,935] INFO Opened connection [connectionId{localValue:82, serverValue:41}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:25,935] INFO Opened connection [connectionId{localValue:83, serverValue:42}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:25,937] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1257012, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000002, setVersion=4, topologyVersion=TopologyVersion{processId=6266c1ccd23913c9480a59ae, counter=8}, lastWriteDate=Mon Apr 25 15:49:16 GMT 2022, lastUpdateTimeNanos=3209052328703} (org.mongodb.driver.cluster)
[2022-04-25 15:49:25,939] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 15:49:25,943] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,945] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,945] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,948] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=10, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,955] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=10, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,956] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=12, connectorIds=[mongoDB-sourcetest], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,956] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,957] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,957] INFO Creating connector mongoDB-sourcetest of type com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,957] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:49:25,957] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:25,958] INFO Instantiated connector mongoDB-sourcetest with version 1.6.1-dirty of type class com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,958] INFO Finished creating connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,958] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,959] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:49:25,959] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:25,970] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Tasks [mongoDB-sourcetest-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,972] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,972] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,972] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,975] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=11, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,980] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=11, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:25,980] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=14, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,980] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 14 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,981] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,981] INFO Creating task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,982] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2022-04-25 15:49:25,982] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:25,982] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.source.MongoSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2022-04-25 15:49:25,983] INFO Instantiated task mongoDB-sourcetest-0 with version 1.6.1-dirty of type com.mongodb.kafka.connect.source.MongoSourceTask (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,984] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:49:25,984] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,984] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:49:25,984] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,984] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,984] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:49:25,985] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:25,985] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:25,985] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongoDB-sourcetest-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:49:25,989] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:49:25,989] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:49:25,990] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:25,990] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:25,990] INFO Kafka startTimeMs: 1650901765989 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:25,992] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:25,992] INFO Starting MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:25,993] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:49:25,994] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 15:49:25,997] INFO Opened connection [connectionId{localValue:85, serverValue:44}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:25,997] INFO Opened connection [connectionId{localValue:84, serverValue:43}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:25,998] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=631742, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000002, setVersion=4, topologyVersion=TopologyVersion{processId=6266c1ccd23913c9480a59ae, counter=8}, lastWriteDate=Mon Apr 25 15:49:16 GMT 2022, lastUpdateTimeNanos=3209114872416} (org.mongodb.driver.cluster)
[2022-04-25 15:49:26,017] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:26,019] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:26,021] INFO Opened connection [connectionId{localValue:86, serverValue:45}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:26,023] INFO Started MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:26,023] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:49:26,024] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:49:32,681] INFO Successfully processed removal of connector 'mongoDB-sourcetest' (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 15:49:32,681] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:32,684] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling connector-only config update by stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:32,684] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:32,684] INFO Scheduled shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 15:49:32,684] INFO Completed shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 15:49:32,685] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:32,687] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:32,698] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=12, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:32,708] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=12, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:32,708] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:32,709] INFO Stopping task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:32,709] WARN Ignoring stop request for unowned connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:32,709] WARN Ignoring await stop request for non-present connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,709] ERROR Graceful stop of task mongoDB-sourcetest-0 failed. (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,711] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 15:49:37,711] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,711] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 15:49:37,711] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,712] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[mongoDB-sourcetest], revokedTaskIds=[mongoDB-sourcetest-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,713] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:49:37,713] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:49:37,713] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:49:37,713] INFO App info kafka.producer for connector-producer-mongoDB-sourcetest-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:37,713] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 16 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,714] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,714] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,714] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,715] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=13, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,723] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=13, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,723] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,724] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 16 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,724] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,727] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 15:49:37,730] INFO Opened connection [connectionId{localValue:87, serverValue:46}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:37,730] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1026975, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000002, setVersion=4, topologyVersion=TopologyVersion{processId=6266c1ccd23913c9480a59ae, counter=8}, lastWriteDate=Mon Apr 25 15:49:34 GMT 2022, lastUpdateTimeNanos=3220847330513} (org.mongodb.driver.cluster)
[2022-04-25 15:49:37,730] INFO Opened connection [connectionId{localValue:88, serverValue:47}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:37,734] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 15:49:37,737] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,740] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,740] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,744] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=14, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,753] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=14, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,753] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 14 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=17, connectorIds=[mongoDB-sourcetest], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,753] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 17 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,753] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,754] INFO Creating connector mongoDB-sourcetest of type com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,754] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:49:37,754] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:37,754] INFO Instantiated connector mongoDB-sourcetest with version 1.6.1-dirty of type class com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,754] INFO Finished creating connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,755] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,756] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:49:37,758] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:37,774] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Tasks [mongoDB-sourcetest-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,779] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,779] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,779] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,781] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=15, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,787] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=15, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 15:49:37,787] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 15 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=19, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,788] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 19 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,789] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,789] INFO Creating task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,789] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2022-04-25 15:49:37,789] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:37,790] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.source.MongoSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2022-04-25 15:49:37,790] INFO Instantiated task mongoDB-sourcetest-0 with version 1.6.1-dirty of type com.mongodb.kafka.connect.source.MongoSourceTask (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,790] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:49:37,790] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,790] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 15:49:37,790] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,790] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,791] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 15:49:37,791] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 15:49:37,791] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 15:49:37,791] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongoDB-sourcetest-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:49:37,793] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:49:37,793] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 15:49:37,793] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:37,793] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:37,793] INFO Kafka startTimeMs: 1650901777793 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:37,795] INFO Starting MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:37,795] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 15:49:37,798] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 15:49:37,807] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 15:49:37,810] INFO Opened connection [connectionId{localValue:90, serverValue:49}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:37,811] INFO Opened connection [connectionId{localValue:89, serverValue:48}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:37,812] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1345807, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000002, setVersion=4, topologyVersion=TopologyVersion{processId=6266c1ccd23913c9480a59ae, counter=8}, lastWriteDate=Mon Apr 25 15:49:34 GMT 2022, lastUpdateTimeNanos=3220927983522} (org.mongodb.driver.cluster)
[2022-04-25 15:49:37,835] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:37,837] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:37,843] INFO Opened connection [connectionId{localValue:91, serverValue:50}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 15:49:37,846] INFO Started MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:37,846] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:49:37,847] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:49:38,036] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:49:38,036] INFO Stopping MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:49:38,039] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 15:49:38,039] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:49:38,039] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:49:38,039] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:49:38,039] INFO App info kafka.producer for connector-producer-mongoDB-sourcetest-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:49:47,774] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:49:57,774] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:50:07,775] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:50:17,754] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:50:27,754] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:50:37,755] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:50:47,734] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:50:57,758] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:51:07,758] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:51:17,737] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:51:27,737] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:51:37,738] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:51:43,689] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoNodeIsRecoveringException: Command failed with error 11600 (InterruptedAtShutdown): 'interrupted at shutdown' on server host.docker.internal:27017. The full response is {"ok": 0.0, "errmsg": "interrupted at shutdown", "code": 11600, "codeName": "InterruptedAtShutdown", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1650901894, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}, "operationTime": {"$timestamp": {"t": 1650901894, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.createSpecialException(ProtocolHelper.java:263)
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:191)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:398)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:51:44,092] INFO An exception occurred when trying to get the next item from the Change Stream (com.mongodb.kafka.connect.source.MongoSourceTask)
com.mongodb.MongoQueryException: Query failed with error code 11600 and error message 'interrupted at shutdown' on server host.docker.internal:27017
	at com.mongodb.internal.operation.QueryHelper.translateCommandException(QueryHelper.java:29)
	at com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:282)
	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:512)
	at com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:270)
	at com.mongodb.internal.operation.QueryBatchCursor.tryHasNext(QueryBatchCursor.java:223)
	at com.mongodb.internal.operation.QueryBatchCursor.lambda$tryNext$0(QueryBatchCursor.java:206)
	at com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:397)
	at com.mongodb.internal.operation.QueryBatchCursor.tryNext(QueryBatchCursor.java:205)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:102)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor$3.apply(ChangeStreamBatchCursor.java:98)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor.resumeableOperation(ChangeStreamBatchCursor.java:195)
	at com.mongodb.internal.operation.ChangeStreamBatchCursor.tryNext(ChangeStreamBatchCursor.java:98)
	at com.mongodb.client.internal.MongoChangeStreamCursorImpl.tryNext(MongoChangeStreamCursorImpl.java:78)
	at com.mongodb.kafka.connect.source.MongoSourceTask.getNextDocument(MongoSourceTask.java:620)
	at com.mongodb.kafka.connect.source.MongoSourceTask.poll(MongoSourceTask.java:223)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:304)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:248)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:51:44,100] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:51:44,102] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:51:44,103] INFO Cluster description not yet available. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster)
[2022-04-25 15:51:47,716] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:51:54,201] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 10 more
[2022-04-25 15:51:57,717] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:52:07,717] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:52:14,082] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:52:14,082] ERROR WorkerSourceTask{id=mongoDB-sourcetest-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask)
com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message}, caused by {java.net.SocketTimeoutException: Read timed out}}]
	at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
	at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
	at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:137)
	at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:94)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:276)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:176)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.execute(ChangeStreamIterableImpl.java:204)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.access$000(ChangeStreamIterableImpl.java:53)
	at com.mongodb.client.internal.ChangeStreamIterableImpl$1.cursor(ChangeStreamIterableImpl.java:129)
	at com.mongodb.client.internal.ChangeStreamIterableImpl$1.cursor(ChangeStreamIterableImpl.java:121)
	at com.mongodb.kafka.connect.source.MongoSourceTask.tryCreateCursor(MongoSourceTask.java:426)
	at com.mongodb.kafka.connect.source.MongoSourceTask.createCursor(MongoSourceTask.java:382)
	at com.mongodb.kafka.connect.source.MongoSourceTask.initializeCursorAndHeartbeatManager(MongoSourceTask.java:369)
	at com.mongodb.kafka.connect.source.MongoSourceTask.getNextDocument(MongoSourceTask.java:615)
	at com.mongodb.kafka.connect.source.MongoSourceTask.poll(MongoSourceTask.java:223)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:304)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:248)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 15:52:14,083] INFO Stopping MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 15:52:14,083] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 15:52:14,086] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:52:14,086] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:52:14,086] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 15:52:14,086] INFO App info kafka.producer for connector-producer-mongoDB-sourcetest-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 15:52:17,696] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:52:27,696] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:52:37,697] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:52:47,676] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:52:57,677] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:53:07,677] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:53:17,656] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:53:27,656] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:53:37,657] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:53:47,635] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:53:57,636] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:54:07,636] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:54:17,616] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:54:27,616] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:54:37,616] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:54:47,596] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:54:57,596] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:55:07,597] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:55:17,575] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:55:27,576] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:55:37,576] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:55:47,555] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:55:57,556] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:56:07,556] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:56:17,535] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:56:27,536] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:56:37,536] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:56:47,515] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:56:57,515] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:57:07,516] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:57:17,495] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:57:27,496] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:57:37,501] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:57:39,701] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:57:39,702] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 15:57:47,480] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:57:57,481] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:58:07,482] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:58:17,461] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:58:27,461] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:58:37,462] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:58:47,441] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:58:57,441] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:59:07,442] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:59:17,421] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:59:27,422] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:59:37,422] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:59:47,401] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 15:59:57,401] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:00:07,402] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:00:17,382] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:00:27,382] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:00:37,382] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:00:47,362] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:00:57,362] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:01:07,363] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:01:12,292] INFO [Producer clientId=producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 16:01:17,342] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:01:27,342] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:01:37,343] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:01:47,322] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:01:57,323] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:02:07,323] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:02:17,303] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:02:27,310] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:02:37,311] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:02:39,579] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 16:02:47,290] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:02:57,290] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:03:07,290] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:03:17,269] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:03:27,270] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:03:37,270] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:03:47,249] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:03:57,250] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:04:07,250] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:04:17,230] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:04:27,230] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:04:37,231] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:04:47,210] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:04:57,211] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:05:07,211] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:05:17,190] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:05:27,191] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:05:37,191] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:05:47,171] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:05:57,171] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:06:07,172] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:06:17,152] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:06:27,152] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:06:37,152] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:06:47,132] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:06:57,132] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:07:07,132] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:07:17,112] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:07:27,112] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:07:37,113] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:07:39,465] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 16:07:47,092] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:07:57,092] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:08:07,093] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:08:15,169] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:08:17,072] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:08:27,073] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:08:37,073] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:08:47,052] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:08:57,053] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:09:07,053] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:09:17,032] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:09:27,033] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:09:37,033] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:09:47,013] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:09:57,013] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:10:07,013] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:10:16,993] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:10:26,993] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:10:36,993] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:10:46,972] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:10:56,973] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:11:06,973] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:11:16,952] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:11:26,953] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:11:36,953] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:11:46,934] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:11:56,934] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:12:06,935] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:12:16,913] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:12:26,913] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:12:36,913] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:12:46,892] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:12:56,893] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:13:06,893] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:13:16,872] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:13:26,873] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:13:36,873] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:13:46,855] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:13:56,860] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:14:06,860] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:14:16,837] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:14:26,837] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:14:36,837] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:14:46,817] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:14:56,817] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:15:06,817] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:15:16,797] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:15:26,797] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:15:36,798] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:15:46,777] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:15:56,777] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:16:06,778] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:16:16,757] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:16:26,757] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:16:36,758] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:16:46,737] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:16:56,739] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:17:06,739] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:17:16,733] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:17:26,734] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:17:36,734] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:17:39,373] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 16:17:46,718] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:17:56,718] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:06,719] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:16,699] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:26,700] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:36,700] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:46,679] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:47,794] INFO Successfully processed removal of connector 'mongoDB-sourcetest' (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 16:18:47,795] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,800] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling connector-only config update by stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,800] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:47,800] INFO Scheduled shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 16:18:47,802] INFO Completed shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 16:18:47,802] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,802] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,804] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=16, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,813] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=16, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,813] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:47,813] WARN Ignoring stop request for unowned connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:47,814] WARN Ignoring await stop request for non-present connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:47,814] INFO Stopping task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:47,814] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,817] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,817] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 16 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=22, connectorIds=[], taskIds=[], revokedConnectorIds=[mongoDB-sourcetest], revokedTaskIds=[mongoDB-sourcetest-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,817] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 22 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,817] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,817] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,817] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,819] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=17, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,826] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=17, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:47,827] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 17 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=22, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,827] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 22 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:47,827] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,188] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 16:18:58,192] INFO Opened connection [connectionId{localValue:97, serverValue:9}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:18:58,192] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1509733} (org.mongodb.driver.cluster)
[2022-04-25 16:18:58,193] INFO Opened connection [connectionId{localValue:98, serverValue:8}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:18:58,193] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 16:18:58,197] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,198] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,198] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,200] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=18, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,207] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=18, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,207] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 18 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=23, connectorIds=[mongoDB-sourcetest], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,208] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 23 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,208] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,208] INFO Creating connector mongoDB-sourcetest of type com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,209] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 16:18:58,209] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:18:58,209] INFO Instantiated connector mongoDB-sourcetest with version 1.6.1-dirty of type class com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,209] INFO Finished creating connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,209] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,210] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 16:18:58,210] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:18:58,223] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Tasks [mongoDB-sourcetest-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,225] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,226] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,226] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,228] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=19, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,235] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=19, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:18:58,235] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 19 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=25, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,236] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 25 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,236] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,236] INFO Creating task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,236] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2022-04-25 16:18:58,236] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:18:58,236] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.source.MongoSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2022-04-25 16:18:58,236] INFO Instantiated task mongoDB-sourcetest-0 with version 1.6.1-dirty of type com.mongodb.kafka.connect.source.MongoSourceTask (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,237] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 16:18:58,237] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,237] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 16:18:58,237] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,237] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,237] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 16:18:58,237] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:18:58,237] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:18:58,238] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongoDB-sourcetest-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 16:18:58,240] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 16:18:58,240] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 16:18:58,240] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 16:18:58,240] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 16:18:58,240] INFO Kafka startTimeMs: 1650903538240 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 16:18:58,242] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:18:58,242] INFO Starting MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:18:58,244] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 16:18:58,244] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 16:18:58,246] INFO Opened connection [connectionId{localValue:100, serverValue:11}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:18:58,246] INFO Opened connection [connectionId{localValue:99, serverValue:10}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:18:58,247] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=746182} (org.mongodb.driver.cluster)
[2022-04-25 16:18:58,254] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:18:58,257] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:18:58,259] INFO Opened connection [connectionId{localValue:101, serverValue:12}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:18:58,264] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:18:58,264] INFO Started MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:18:58,264] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:58,265] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:18:58,270] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:18:58,272] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:18:58,273] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:03,269] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:03,271] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:03,272] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:03,276] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:03,277] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:03,279] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:08,243] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:19:08,279] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:08,281] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:08,283] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:08,286] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:08,288] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:08,289] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,268] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,270] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,272] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,286] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,288] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,289] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,293] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,295] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:13,296] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:18,224] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:19:18,297] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:18,300] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:18,301] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:18,305] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:18,307] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:18,308] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:23,316] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:23,317] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:23,318] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:23,322] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:23,323] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:23,324] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:28,225] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:19:28,321] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:28,325] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:28,327] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:28,329] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:28,330] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:28,332] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:33,331] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:33,332] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:33,334] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:33,337] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:33,338] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:33,339] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:38,227] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:19:38,340] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:38,341] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:38,342] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:38,345] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:38,346] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:38,347] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,326] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,327] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,328] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,345] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,347] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,349] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,353] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,354] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:43,356] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:48,209] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:19:48,352] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:48,353] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:48,355] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:48,366] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:48,367] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:48,369] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:53,358] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:53,359] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:53,361] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:53,367] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:53,368] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:53,370] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:58,211] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:19:58,365] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:58,367] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:58,369] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:58,374] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:58,375] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:19:58,377] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:03,372] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:03,373] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:03,375] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:03,380] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:03,381] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:03,382] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:08,213] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:20:08,380] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:08,384] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:08,385] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:08,389] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:08,390] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:08,392] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,367] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,368] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,369] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,395] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,395] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,397] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,400] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,401] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:13,402] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:18,193] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:20:18,405] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:18,410] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:18,411] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:18,415] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:18,416] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:18,418] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:23,419] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:23,420] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:23,423] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:23,426] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:23,427] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:23,428] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:28,194] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:20:28,428] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:28,430] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:28,432] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:28,435] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:28,436] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:28,438] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:33,436] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:33,437] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:33,438] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:33,442] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:33,443] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:33,445] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:38,195] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:20:38,443] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:38,444] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:38,445] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:38,448] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:38,449] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:38,451] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,429] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,430] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,431] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,450] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,451] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,452] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,455] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,456] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:43,457] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:48,176] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:20:48,457] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:48,459] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:48,461] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:48,464] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:48,465] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:48,466] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:53,466] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:53,468] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:53,469] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:53,473] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:53,474] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:53,475] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:58,178] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:20:58,476] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:58,478] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:58,479] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:58,483] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:58,484] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:20:58,485] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:03,484] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:03,485] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:03,486] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:03,489] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:03,490] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:03,491] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:08,180] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:21:08,491] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:08,493] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:08,494] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:08,499] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:08,500] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:08,502] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,479] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,480] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,481] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,499] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,500] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,502] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,505] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,506] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:13,507] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:18,160] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:21:18,509] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:18,510] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:18,512] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:18,515] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:18,516] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:18,518] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:23,517] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:23,518] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:23,520] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:23,524] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:23,525] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:23,526] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:28,162] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:21:28,527] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:28,528] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:28,530] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:28,534] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:28,534] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:28,536] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:33,536] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:33,538] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:33,540] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:33,544] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:33,545] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:33,546] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:38,163] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:21:38,548] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:38,549] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:38,551] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:38,554] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:38,555] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:38,556] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,535] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,536] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,538] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,557] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,558] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,560] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,564] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,564] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:43,566] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:48,144] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:21:48,566] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:48,568] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:48,569] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:48,573] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:48,574] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:48,576] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:53,574] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:53,574] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:53,576] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:53,579] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:53,580] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:53,582] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:58,145] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:21:58,581] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:58,583] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:58,585] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:58,588] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:58,589] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:21:58,590] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:03,591] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:03,592] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:03,594] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:03,597] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:03,597] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:03,599] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:08,147] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:22:08,600] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:08,601] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:08,603] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:08,606] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:08,607] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:08,608] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,588] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,589] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,591] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,608] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,609] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,611] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,614] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,615] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:13,616] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:18,128] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:22:18,619] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:18,621] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:18,622] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:18,626] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:18,627] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:18,629] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:23,627] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:23,628] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:23,629] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:23,632] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:23,633] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:23,634] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:28,129] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:22:28,637] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:28,638] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:28,641] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:28,644] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:28,645] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:28,646] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:33,649] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:33,650] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:33,652] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:33,655] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:33,656] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:33,657] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:38,130] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:22:38,659] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:38,660] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:38,662] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:38,665] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:38,666] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:38,668] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,645] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,646] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,648] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,668] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,669] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,670] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,674] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,675] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:43,676] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:48,110] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:22:48,676] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:48,677] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:48,679] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:48,682] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:48,682] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:48,684] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:53,682] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:53,683] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:53,684] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:53,686] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:53,687] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:53,688] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:58,112] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:22:58,688] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:58,689] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:58,690] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:58,693] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:58,694] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:22:58,696] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:03,693] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:03,694] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:03,695] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:03,698] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:03,699] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:03,700] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:08,113] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:23:08,699] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:08,701] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:08,702] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:08,706] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:08,706] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:08,708] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,686] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,688] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,689] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,707] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,708] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,710] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,713] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,714] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:13,715] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:18,093] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:23:18,715] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:18,716] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:18,718] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:18,722] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:18,723] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:18,724] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:23,723] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:23,724] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:23,725] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:23,728] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:23,729] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:23,730] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:28,094] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:23:28,729] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:28,732] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:28,734] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:28,737] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:28,737] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:28,739] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:33,739] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:33,740] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:33,741] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:33,746] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:33,747] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:33,749] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:38,096] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:23:38,744] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:38,746] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:38,748] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:38,751] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:38,752] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:38,753] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,730] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,731] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,732] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,751] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,752] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,753] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,758] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,758] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:43,759] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:48,077] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:23:48,759] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:48,760] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:48,761] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:48,764] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:48,764] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:48,766] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:53,765] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:53,766] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:53,768] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:53,771] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:53,771] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:53,773] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:58,079] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:23:58,789] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:58,818] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:58,878] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:58,909] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:58,925] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:23:58,950] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:03,890] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:03,891] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:03,892] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:03,897] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:03,897] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:03,899] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:08,083] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:24:08,897] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:08,898] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:08,899] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:08,902] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:08,903] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:08,904] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,884] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,885] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,886] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,902] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,903] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,905] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,908] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,908] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:13,909] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:18,065] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:24:18,910] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:18,913] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:18,915] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:18,918] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:18,920] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:18,921] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:23,920] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:23,921] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:23,923] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:23,927] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:23,929] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:23,930] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:28,066] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:24:28,929] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:28,932] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:28,934] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:28,938] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:28,939] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:28,940] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:33,940] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:33,941] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:33,943] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:33,947] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:33,947] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:33,949] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:38,071] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:24:38,949] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:38,950] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:38,953] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:38,956] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:38,957] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:38,958] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,938] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,939] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,941] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,957] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,958] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,962] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,965] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,966] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:43,967] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:48,051] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:24:48,968] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:48,969] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:48,970] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:48,973] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:48,974] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:48,975] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:53,978] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:53,979] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:53,981] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:53,984] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:53,985] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:53,987] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:58,052] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:24:58,985] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:58,987] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:58,990] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:58,993] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:58,994] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:24:58,995] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:03,995] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:03,996] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:03,997] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:04,001] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:04,001] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:04,003] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:08,053] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:25:09,002] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:09,004] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:09,007] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:09,010] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:09,010] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:09,012] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:13,990] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:13,991] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:13,993] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:14,010] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:14,011] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:14,012] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:14,014] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:14,015] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:14,016] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:18,033] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:25:19,017] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:19,018] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:19,020] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:19,025] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:19,026] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:19,028] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:24,024] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:24,025] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:24,027] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:24,030] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:24,031] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:24,032] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:28,034] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:25:29,033] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:29,034] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:29,036] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:29,039] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:29,040] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:29,042] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:34,041] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:34,042] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:34,044] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:34,047] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:34,048] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:34,050] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:38,035] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:25:39,050] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:39,052] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:39,054] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:39,057] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:39,058] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:39,059] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,038] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,039] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,040] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,058] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,059] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,060] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,064] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,065] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:44,067] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:48,014] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:25:49,065] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:49,067] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:49,068] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:49,072] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:49,073] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:49,074] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:54,074] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:54,075] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:54,076] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:54,079] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:54,080] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:54,081] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:58,015] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:25:59,081] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:59,082] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:59,084] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:59,086] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:59,087] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:25:59,088] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:04,092] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:04,092] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:04,094] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:04,098] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:04,098] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:04,100] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:08,016] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:26:09,098] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:09,100] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:09,101] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:09,105] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:09,105] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:09,107] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,085] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,086] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,088] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,106] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,107] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,109] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,113] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,114] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:14,116] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:17,996] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:26:19,114] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:19,116] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:19,118] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:19,121] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:19,122] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:19,123] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:24,123] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:24,124] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:24,125] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:24,133] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:24,133] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:24,135] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:27,997] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:26:29,129] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:29,130] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:29,131] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:29,134] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:29,135] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:29,136] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:34,135] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:34,136] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:34,138] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:34,141] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:34,142] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:34,143] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:37,998] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:26:39,143] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:39,144] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:39,146] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:39,149] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:39,150] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:39,151] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,130] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,130] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,133] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,149] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,150] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,151] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,155] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,155] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:44,157] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:47,977] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:26:49,155] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:49,156] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:49,157] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:49,160] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:49,160] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:49,161] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:54,163] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:54,164] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:54,165] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:54,169] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:54,170] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:54,171] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:57,978] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:26:59,169] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:59,172] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:59,174] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:59,177] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:59,178] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:26:59,179] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:04,180] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:04,181] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:04,184] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:04,188] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:04,188] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:04,190] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:07,980] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:27:09,190] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:09,192] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:09,194] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:09,197] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:09,198] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:09,199] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,179] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,179] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,181] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,198] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,199] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,200] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,203] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,204] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:14,205] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:17,959] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:27:19,206] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:19,208] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:19,209] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:19,212] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:19,213] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:19,214] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:24,214] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:24,215] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:24,217] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:24,221] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:24,222] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:24,224] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:27,960] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:27:29,224] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:29,225] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:29,227] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:29,230] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:29,230] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:29,231] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:34,232] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:34,233] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:34,235] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:34,238] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:34,239] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:34,241] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:37,961] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:27:39,241] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:39,242] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:39,244] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:39,247] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:39,248] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:39,249] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,226] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,227] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,228] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,248] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,249] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,251] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,254] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,255] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:44,256] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:47,940] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:27:49,257] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:49,258] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:49,260] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:49,264] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:49,265] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:49,266] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:54,266] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:54,267] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:54,268] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:54,271] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:54,272] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:54,273] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:57,941] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:27:58,276] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 16:27:59,278] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:59,279] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:59,282] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:59,288] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:59,289] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:27:59,292] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:04,286] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:04,286] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:04,287] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:04,290] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:04,291] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:04,292] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:07,942] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:28:09,292] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:09,294] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:09,295] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:09,299] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:09,299] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:09,301] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,280] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,281] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,283] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,299] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,299] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,301] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,304] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,304] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:14,306] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:17,922] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:28:19,306] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:19,307] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:19,308] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:19,311] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:19,311] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:19,313] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:24,313] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:24,314] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:24,316] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:24,319] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:24,320] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:24,322] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:27,923] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:28:29,319] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:29,321] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:29,322] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:29,325] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:29,326] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:29,328] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:34,327] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:34,328] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:34,330] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:34,332] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:34,333] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:34,335] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:37,923] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:28:39,336] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:39,337] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:39,339] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:39,343] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:39,343] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:39,345] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,323] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,323] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,325] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,342] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,343] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,344] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,347] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,347] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:44,349] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:47,903] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:28:49,346] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:49,347] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:49,348] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:49,351] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:49,351] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:49,354] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:54,354] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:54,355] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:54,358] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:54,364] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:54,365] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:54,366] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:57,904] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:28:59,363] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:59,364] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:59,366] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:59,369] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:59,369] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:28:59,370] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:04,370] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:04,371] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:04,372] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:04,375] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:04,376] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:04,377] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:07,905] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:29:09,378] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:09,379] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:09,381] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:09,383] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:09,384] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:09,386] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,366] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,367] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,368] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,384] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,385] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,387] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,390] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,390] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:14,391] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:17,886] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:29:19,393] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:19,395] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:19,396] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:19,399] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:19,399] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:19,401] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:24,404] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:24,404] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:24,406] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:24,409] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:24,410] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:24,411] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:27,887] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:29:29,416] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:29,418] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:29,419] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:29,423] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:29,423] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:29,425] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:34,423] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:34,424] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:34,425] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:34,428] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:34,429] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:34,430] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:37,888] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:29:39,432] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:39,433] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:39,434] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:39,438] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:39,438] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:39,439] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,417] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,418] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,419] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,439] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,440] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,441] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,444] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,444] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:44,446] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:47,868] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:29:49,443] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:49,444] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:49,446] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:49,448] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:49,448] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:49,449] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:54,451] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:54,451] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:54,454] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:54,456] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:54,457] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:54,458] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:57,869] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:29:59,461] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:59,462] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:59,479] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:59,485] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:59,485] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:29:59,549] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:04,484] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:04,485] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:04,486] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:04,489] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:04,489] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:04,490] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:07,870] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:30:09,492] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:09,493] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:09,494] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:09,497] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:09,498] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:09,499] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,477] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,478] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,479] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,500] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,501] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,503] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,506] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,507] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:14,509] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:17,850] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:30:19,508] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:19,510] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:19,512] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:19,515] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:19,516] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:19,517] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:24,518] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:24,520] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:24,522] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:24,526] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:24,526] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:24,528] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:27,851] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:30:29,528] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:29,529] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:29,531] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:29,534] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:29,535] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:29,536] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:34,536] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:34,537] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:34,539] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:34,541] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:34,542] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:34,543] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:37,852] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:30:39,544] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:39,546] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:39,547] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:39,550] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:39,551] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:39,552] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,531] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,532] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,534] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,549] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,550] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,551] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,554] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,554] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:44,557] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:47,832] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:30:49,557] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:49,559] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:49,560] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:49,563] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:49,564] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:49,565] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:54,565] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:54,566] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:54,567] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:54,569] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:54,570] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:54,570] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:57,832] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:30:59,573] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:59,575] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:59,576] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:59,579] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:59,580] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:30:59,581] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:04,582] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:04,583] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:04,586] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:04,589] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:04,589] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:04,590] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:07,834] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:31:09,591] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:09,592] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:09,594] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:09,597] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:09,598] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:09,599] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,579] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,580] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,582] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,598] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,599] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,600] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,603] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,604] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:14,605] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:17,814] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:31:19,604] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:19,606] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:19,607] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:19,610] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:19,610] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:19,612] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:24,613] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:24,614] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:24,615] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:24,619] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:24,619] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:24,620] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:27,814] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:31:29,621] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:29,622] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:29,624] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:29,627] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:29,627] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:29,629] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:34,628] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:34,630] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:34,632] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:34,636] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:34,637] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:34,638] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:37,815] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:31:39,637] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:39,638] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:39,641] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:39,645] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:39,645] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:39,647] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,625] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,625] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,627] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,647] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,648] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,649] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,654] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,655] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:44,656] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:47,795] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:31:49,654] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:49,657] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:49,659] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:49,661] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:49,662] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:49,663] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:54,662] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:54,663] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:54,664] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:54,667] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:54,667] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:54,668] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:57,795] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:31:59,668] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:59,669] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:59,671] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:59,677] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:59,678] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:31:59,679] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:04,680] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:04,680] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:04,682] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:04,685] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:04,685] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:04,687] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:07,797] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:32:09,684] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:09,685] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:09,686] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:09,689] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:09,689] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:09,690] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,668] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,669] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,670] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,689] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,689] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,691] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,693] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,693] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:14,695] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:17,777] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:32:19,694] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:19,695] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:19,696] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:19,699] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:19,699] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:19,701] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:24,699] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:24,700] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:24,701] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:24,715] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:24,715] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:24,717] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:27,777] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:32:29,703] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:29,704] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:29,705] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:29,709] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:29,709] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:29,711] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:34,708] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:34,708] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:34,710] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:34,712] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:34,712] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:34,713] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:37,778] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:32:39,713] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:39,714] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:39,716] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:39,719] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:39,719] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:39,721] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,698] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,698] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,699] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,719] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,720] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,721] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,723] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,724] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:44,725] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:47,758] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:32:49,724] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:49,725] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:49,727] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:49,729] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:49,730] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:49,731] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:54,730] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:54,730] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:54,732] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:54,734] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:54,735] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:54,736] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:57,758] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:32:59,735] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:59,736] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:59,738] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:59,741] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:59,741] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:32:59,743] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:04,740] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:04,741] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:04,743] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:04,745] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:04,745] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:04,747] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:07,759] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:33:09,746] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:09,747] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:09,749] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:09,751] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:09,752] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:09,753] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,731] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,732] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,733] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,752] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,752] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,753] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,756] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,756] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:14,757] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:17,740] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:33:19,756] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:19,757] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:19,758] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:19,761] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:19,762] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:19,763] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:24,761] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:24,762] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:24,764] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:24,767] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:24,768] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:24,783] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:27,740] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:33:29,766] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:29,769] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:29,772] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:29,784] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:29,785] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:29,787] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:34,785] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:34,786] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:34,787] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:34,790] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:34,791] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:34,792] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:37,741] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:33:39,790] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:39,791] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:39,793] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:39,797] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:39,798] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:39,799] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,772] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,773] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,774] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,800] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,800] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,802] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,804] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,805] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:44,806] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:47,718] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:33:49,807] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:49,808] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:49,810] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:49,812] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:49,812] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:49,814] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:54,812] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:54,812] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:54,814] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:54,818] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:54,819] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:54,820] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:57,719] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:33:59,817] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:59,818] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:59,819] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:59,821] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:59,821] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:33:59,822] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:04,826] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:04,826] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:04,828] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:04,830] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:04,831] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:04,832] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:07,719] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:34:09,832] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:09,833] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:09,834] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:09,837] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:09,837] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:09,838] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,809] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,810] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,811] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,837] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,837] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,839] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,841] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,842] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:14,843] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:17,692] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:34:19,842] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:19,843] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:19,845] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:19,848] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:19,849] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:19,850] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:24,848] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:24,848] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:24,850] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:24,852] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:24,852] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:24,853] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:27,693] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:34:29,853] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:29,854] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:29,856] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:29,859] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:29,860] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:29,861] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:34,859] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:34,859] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:34,861] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:34,863] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:34,863] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:34,865] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:37,694] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:34:39,871] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:39,872] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:39,874] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:39,876] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:39,877] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:39,878] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,855] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,856] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,857] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,877] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,878] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,879] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,882] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,882] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:44,884] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:47,672] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:34:49,882] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:49,883] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:49,884] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:49,886] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:49,887] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:49,889] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:54,887] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:54,887] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:54,889] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:54,891] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:54,892] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:54,893] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:57,676] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:34:59,892] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:59,893] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:59,894] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:59,897] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:59,897] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:34:59,899] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:04,897] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:04,898] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:04,899] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:04,902] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:04,902] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:04,904] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:07,677] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:35:09,904] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:09,904] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:09,906] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:09,908] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:09,909] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:09,911] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,887] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,888] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,889] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,909] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,910] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,912] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,914] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,915] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:14,916] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:17,657] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:35:19,914] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:19,915] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:19,917] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:19,922] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:19,922] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:19,923] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:24,920] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:24,921] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:24,922] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:24,924] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:24,924] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:24,925] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:27,657] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:35:29,926] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:29,927] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:29,928] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:29,931] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:29,931] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:29,932] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:34,931] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:34,932] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:34,934] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:34,937] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:34,938] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:34,940] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:37,658] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:35:39,936] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:39,937] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:39,939] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:39,942] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:39,943] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:39,945] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,926] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,927] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,928] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,942] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,942] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,944] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,948] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,948] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:44,949] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:47,638] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:35:49,948] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:49,949] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:49,950] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:49,954] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:49,954] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:49,956] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:54,953] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:54,953] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:54,956] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:54,963] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:54,964] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:54,966] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:57,641] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:35:59,959] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:59,959] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:59,961] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:59,963] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:59,963] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:35:59,964] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:04,965] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:04,965] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:04,967] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:04,973] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:04,974] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:04,975] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:07,642] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:36:09,970] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:09,970] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:09,972] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:09,975] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:09,975] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:09,976] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,953] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,953] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,955] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,974] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,975] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,976] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,979] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,980] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:14,981] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:17,622] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:36:19,980] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:19,981] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:19,983] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:19,985] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:19,986] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:19,987] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:24,986] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:24,986] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:24,987] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:24,989] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:24,990] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:25,000] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:27,623] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:36:29,990] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:29,993] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:29,994] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:30,002] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:30,002] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:30,004] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:34,999] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:35,000] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:35,001] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:35,004] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:35,004] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:35,005] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:37,624] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:36:40,004] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:40,005] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:40,006] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:40,008] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:40,008] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:40,009] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:44,991] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:44,992] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:44,995] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:45,009] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:45,010] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:45,012] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:45,014] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:45,014] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:45,016] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:47,604] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:36:50,016] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:50,017] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:50,019] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:50,021] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:50,022] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:50,023] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:55,021] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:55,022] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:55,023] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:55,025] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:55,025] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:55,026] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:36:57,605] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:37:00,026] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:00,027] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:00,029] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:00,031] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:00,031] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:00,032] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:05,035] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:05,036] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:05,037] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:05,040] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:05,040] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:05,042] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:07,608] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:37:10,043] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:10,044] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:10,045] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:10,048] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:10,049] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:10,050] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,030] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,030] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,032] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,050] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,051] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,053] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,055] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,056] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:15,057] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:17,587] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:37:20,056] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:20,056] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:20,057] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:20,059] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:20,059] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:20,060] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:25,063] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:25,064] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:25,066] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:25,070] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:25,071] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:25,072] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:27,588] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:37:30,071] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:30,072] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:30,074] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:30,077] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:30,077] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:30,079] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:35,079] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:35,080] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:35,083] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:35,086] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:35,086] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:35,087] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:37,589] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:37:40,088] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:40,089] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:40,091] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:40,093] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:40,094] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:40,095] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,074] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,075] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,077] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,093] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,094] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,095] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,098] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,099] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:45,100] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:47,568] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:37:50,101] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:50,102] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:50,103] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:50,106] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:50,107] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:50,109] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:55,109] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:55,110] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:55,112] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:55,117] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:55,118] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:55,121] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:37:57,569] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:38:00,115] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:00,116] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:00,119] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:00,121] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:00,122] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:00,123] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:05,123] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:05,123] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:05,125] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:05,127] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:05,128] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:05,129] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:07,570] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:38:10,130] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:10,131] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:10,133] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:10,136] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:10,136] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:10,138] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,117] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,117] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,120] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,136] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,136] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,137] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,139] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,140] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:15,141] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:16,041] INFO Successfully processed removal of connector 'mongoDB-sourcetest' (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 16:38:16,041] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:16,043] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling connector-only config update by stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:16,044] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:38:16,045] INFO Scheduled shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 16:38:16,046] INFO Completed shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 16:38:16,048] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:16,048] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:16,052] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=20, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:16,059] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=20, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:16,060] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:38:16,060] WARN Ignoring stop request for unowned connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:38:16,060] WARN Ignoring await stop request for non-present connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:38:16,060] INFO Stopping task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:38:20,142] INFO Watching for collection changes on 'testdb.newcol' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:20,143] INFO Resuming the change stream after the previous offset: {"_data": "826266C347000000012B022C0100296E5A10049B4056834AB047BEA3ACCF667D18FF7546645F696400646266C347A608DA786D00EBB70004"} (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:20,144] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:20,144] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:38:20,145] INFO Stopping MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:38:20,148] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 16:38:20,149] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 16:38:20,149] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 16:38:20,149] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 16:38:20,149] INFO App info kafka.producer for connector-producer-mongoDB-sourcetest-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 16:38:20,151] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:20,153] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:20,153] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 20 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=27, connectorIds=[], taskIds=[], revokedConnectorIds=[mongoDB-sourcetest], revokedTaskIds=[mongoDB-sourcetest-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:20,153] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 27 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:20,154] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:20,154] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:20,154] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:20,155] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=21, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:20,160] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=21, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:38:20,160] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 21 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=27, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:20,160] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 27 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:38:20,161] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,402] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 16:39:19,409] INFO Opened connection [connectionId{localValue:103, serverValue:13}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:39:19,409] INFO Opened connection [connectionId{localValue:102, serverValue:14}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:39:19,410] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1694415} (org.mongodb.driver.cluster)
[2022-04-25 16:39:19,415] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 16:39:19,417] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,419] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,419] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,421] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=22, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,429] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=22, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,429] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 22 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=28, connectorIds=[mongoDB-sourcetest], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,429] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 28 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,430] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,430] INFO Creating connector mongoDB-sourcetest of type com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,431] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 16:39:19,431] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:39:19,431] INFO Instantiated connector mongoDB-sourcetest with version 1.6.1-dirty of type class com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,431] INFO Finished creating connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,432] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,432] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 16:39:19,432] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:39:19,468] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Tasks [mongoDB-sourcetest-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,470] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,471] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,471] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,472] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=23, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,481] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=23, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 16:39:19,481] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 23 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', leaderUrl='http://connect:8083/', offset=30, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,481] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 30 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,481] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,482] INFO Creating task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,482] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2022-04-25 16:39:19,482] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:39:19,482] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.source.MongoSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2022-04-25 16:39:19,482] INFO Instantiated task mongoDB-sourcetest-0 with version 1.6.1-dirty of type com.mongodb.kafka.connect.source.MongoSourceTask (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,484] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 16:39:19,484] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,484] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 16:39:19,484] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,484] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,485] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 16:39:19,485] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 16:39:19,485] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 16:39:19,485] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongoDB-sourcetest-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 16:39:19,491] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 16:39:19,491] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 16:39:19,491] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 16:39:19,491] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 16:39:19,491] INFO Kafka startTimeMs: 1650904759491 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 16:39:19,494] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 16:39:19,494] INFO Starting MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:19,496] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 16:39:19,497] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 16:39:19,511] INFO Opened connection [connectionId{localValue:104, serverValue:15}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:39:19,511] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=13410710} (org.mongodb.driver.cluster)
[2022-04-25 16:39:19,512] INFO Opened connection [connectionId{localValue:105, serverValue:16}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:39:19,532] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:19,533] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:19,535] INFO Opened connection [connectionId{localValue:106, serverValue:17}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:39:19,536] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:19,536] INFO Started MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:19,537] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:39:19,537] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:39:19,542] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:19,542] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:19,544] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:24,542] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:24,542] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:24,544] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:24,549] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:24,550] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:24,551] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:29,493] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:39:29,549] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:29,550] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:29,551] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:29,555] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:29,556] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:29,557] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:34,557] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:34,557] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:34,559] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:34,563] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:34,564] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:34,565] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:39,494] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:39:39,565] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:39,566] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:39,568] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:39,571] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:39,572] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:39,572] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,555] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,555] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,557] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,573] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,573] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,575] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,582] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,583] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:44,584] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:49,474] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:39:49,579] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:49,580] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:49,582] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:49,589] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:49,589] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:49,591] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:54,587] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:54,588] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:54,590] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:54,597] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:54,598] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:54,599] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:59,474] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:39:59,594] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:59,595] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:59,596] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:59,600] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:59,600] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:39:59,601] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:04,600] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:04,601] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:04,602] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:04,607] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:04,607] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:04,608] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:09,475] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:40:09,607] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:09,608] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:09,609] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:09,612] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:09,613] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:09,614] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,593] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,593] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,595] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,613] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,614] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,616] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,619] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,620] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:14,621] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:19,455] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:40:19,621] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:19,622] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:19,623] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:19,626] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:19,627] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:19,629] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:24,630] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:24,631] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:24,633] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:24,638] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:24,638] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:24,640] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:29,456] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:40:29,637] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:29,638] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:29,640] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:29,655] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:29,656] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:29,657] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:34,645] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:34,646] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:34,648] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:34,653] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:34,654] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:34,654] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:39,457] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:40:39,661] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:39,663] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:39,664] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:39,669] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:39,669] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:39,671] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,648] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,648] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,651] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,672] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,673] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,674] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,686] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,686] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:44,687] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:49,437] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:40:49,680] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:49,681] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:49,682] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:49,686] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:49,687] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:49,689] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:54,691] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:54,691] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:54,692] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:54,696] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:54,697] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:54,698] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:59,437] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:40:59,696] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:59,697] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:59,699] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:59,702] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:59,702] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:40:59,703] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:04,708] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:04,709] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:04,710] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:04,716] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:04,716] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:04,718] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:09,438] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:41:09,722] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:09,723] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:09,724] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:09,746] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:09,746] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:09,748] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,708] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,709] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,710] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,735] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,735] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,737] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,741] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,742] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:14,743] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:19,418] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:41:19,742] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:19,743] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:19,744] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:19,750] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:19,750] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:19,751] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:24,750] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:24,750] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:24,752] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:24,755] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:24,756] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:24,757] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:29,419] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:41:29,763] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:29,763] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:29,766] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:29,769] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:29,769] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:29,770] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:34,772] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:34,772] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:34,775] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:34,779] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:34,779] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:34,780] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:39,419] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:41:39,781] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:39,782] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:39,784] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:39,787] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:39,787] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:39,788] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,767] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,768] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,770] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,788] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,789] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,804] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,808] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,808] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:44,809] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:49,399] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:41:49,812] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:49,813] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:49,814] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:49,818] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:49,819] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:49,820] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:54,821] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:54,822] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:54,823] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:54,832] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:54,832] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:54,833] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:59,400] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:41:59,832] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:59,833] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:59,834] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:59,841] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:59,842] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:41:59,843] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:04,841] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:04,842] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:04,843] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:04,852] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:04,852] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:04,853] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:09,401] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:42:09,851] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:09,851] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:09,853] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:09,857] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:09,857] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:09,858] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,838] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,839] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,840] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,858] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,859] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,860] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,864] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,864] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:14,871] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:19,380] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:42:19,864] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:19,865] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:19,867] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:19,873] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:19,873] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:19,874] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:24,875] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:24,876] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:24,878] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:24,883] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:24,884] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:24,885] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:29,381] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:42:29,893] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:29,894] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:29,897] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:29,905] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:29,905] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:29,907] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:34,902] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:34,903] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:34,904] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:34,908] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:34,908] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:34,910] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:39,381] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:42:39,909] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:39,911] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:39,912] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:39,917] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:39,918] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:39,919] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,895] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,896] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,897] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,920] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,921] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,922] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,927] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,928] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:44,929] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:49,361] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:42:49,928] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:49,929] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:49,930] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:49,934] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:49,935] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:49,936] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:54,938] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:54,938] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:54,939] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:54,944] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:54,944] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:54,945] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:59,362] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:42:59,945] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:59,946] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:59,947] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:59,951] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:59,951] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:42:59,953] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:04,952] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:04,952] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:04,953] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:04,959] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:04,959] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:04,960] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:09,362] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:43:09,961] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:09,962] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:09,964] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:09,968] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:09,968] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:09,970] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,948] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,949] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,952] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,971] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,971] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,973] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,983] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,983] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:14,985] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:19,342] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:43:19,983] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:19,984] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:19,985] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:19,993] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:19,994] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:19,995] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:24,990] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:24,991] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:24,992] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:24,996] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:24,997] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:24,998] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:29,343] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:43:29,996] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:29,996] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:29,998] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:30,002] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:30,002] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:30,003] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:35,003] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:35,003] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:35,004] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:35,008] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:35,008] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:35,010] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:39,344] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:43:40,008] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:40,009] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:40,011] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:40,015] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:40,015] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:40,016] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:44,994] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:44,995] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:44,997] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:45,014] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:45,015] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:45,016] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:45,021] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:45,021] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:45,022] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:49,324] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:43:50,020] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:50,021] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:50,022] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:50,025] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:50,026] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:50,027] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:55,026] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:55,027] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:55,029] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:55,034] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:55,034] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:55,036] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:43:59,324] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:44:00,034] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:00,035] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:00,036] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:00,052] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:00,053] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:00,055] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:05,041] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:05,041] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:05,043] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:05,047] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:05,048] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:05,050] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:09,325] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:44:10,047] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:10,048] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:10,050] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:10,055] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:10,055] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:10,057] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,038] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,039] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,040] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,054] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,055] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,056] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,062] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,062] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:15,066] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:19,305] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:44:20,070] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:20,071] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:20,073] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:20,077] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:20,077] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:20,081] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:25,077] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:25,078] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:25,079] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:25,083] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:25,083] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:25,085] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:29,309] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:44:30,084] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:30,085] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:30,086] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:30,092] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:30,092] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:30,093] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:35,091] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:35,092] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:35,094] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:35,097] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:35,098] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:35,103] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:39,310] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:44:40,098] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:40,099] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:40,101] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:40,106] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:40,107] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:40,109] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,084] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,085] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,098] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,106] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,107] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,108] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,113] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,114] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:45,116] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:49,290] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:44:50,119] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:50,120] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:50,121] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:50,126] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:50,127] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:50,128] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:55,126] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:55,127] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:55,128] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:55,132] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:55,133] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:55,134] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:44:59,291] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:45:00,134] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:00,135] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:00,138] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:00,142] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:00,143] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:00,144] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:05,142] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:05,142] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:05,144] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:05,149] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:05,150] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:05,151] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:09,291] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:45:10,163] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:10,164] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:10,167] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:10,178] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:10,179] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:10,181] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,150] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,151] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,153] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,171] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,172] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,173] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,177] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,177] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:15,179] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:19,271] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:45:20,178] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:20,179] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:20,181] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:20,186] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:20,187] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:20,189] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:25,185] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:25,186] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:25,189] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:25,194] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:25,195] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:25,197] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:29,271] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:45:30,199] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:30,203] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:30,205] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:30,212] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:30,212] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:30,214] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:35,210] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:35,211] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:35,212] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:35,216] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:35,217] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:35,218] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:39,272] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:45:40,217] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:40,217] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:40,219] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:40,223] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:40,223] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:40,224] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:45,270] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:45,272] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:45,275] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:45,283] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:45,284] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:45,285] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:49,252] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:45:50,280] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:50,280] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:50,282] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:50,286] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:50,287] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:50,288] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:55,287] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:55,287] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:55,289] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:55,294] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:55,294] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:55,297] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:45:59,253] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:46:00,293] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:00,293] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:00,295] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:00,299] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:00,299] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:00,301] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:05,300] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:05,301] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:05,302] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:05,307] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:05,308] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:05,310] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:09,254] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:46:10,285] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,286] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,289] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,307] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,308] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,314] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,318] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,319] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:10,321] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:15,321] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:15,322] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:15,324] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:15,328] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:15,329] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:15,331] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:19,233] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:46:20,329] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:20,330] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:20,331] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:20,335] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:20,336] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:20,337] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:25,336] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:25,336] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:25,338] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:25,342] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:25,343] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:25,344] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:29,234] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:46:30,342] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:30,343] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:30,345] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:30,349] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:30,349] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:30,351] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:35,349] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:35,349] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:35,351] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:35,355] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:35,356] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:35,357] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:39,235] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:46:40,334] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,336] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,338] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,355] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,355] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,357] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,360] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,361] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:40,362] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:45,361] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:45,362] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:45,363] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:45,366] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:45,367] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:45,368] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:49,214] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:46:50,367] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:50,368] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:50,370] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:50,373] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:50,373] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:50,374] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:55,383] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:55,383] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:55,384] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:55,388] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:55,389] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:55,390] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:46:59,215] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:47:00,389] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:00,390] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:00,392] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:00,406] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:00,406] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:00,408] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:05,396] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:05,397] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:05,398] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:05,401] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:05,402] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:05,404] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:09,216] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:47:10,387] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,388] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,390] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,402] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,403] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,404] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,410] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,410] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:10,412] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:15,408] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:15,408] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:15,410] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:15,414] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:15,414] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:15,415] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:19,195] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:47:20,415] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:20,416] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:20,418] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:20,423] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:20,423] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:20,425] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:25,422] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:25,423] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:25,424] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:25,428] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:25,428] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:25,430] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:29,196] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:47:30,429] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:30,430] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:30,431] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:30,444] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:30,445] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:30,447] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:35,435] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:35,436] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:35,438] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:35,442] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:35,443] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:35,444] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:39,197] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:47:40,443] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:40,444] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:40,447] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:40,453] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:40,453] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:40,455] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:45,452] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:45,453] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:45,454] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:45,462] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:45,462] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:45,463] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:49,177] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:47:50,461] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:50,462] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:50,464] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:50,468] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:50,468] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:50,471] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:55,494] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:55,495] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:55,497] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:55,502] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:55,503] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:55,505] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:47:59,177] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:48:00,501] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:00,503] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:00,504] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:00,510] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:00,511] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:00,512] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:05,511] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:05,512] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:05,513] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:05,517] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:05,517] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:05,518] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:09,178] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:48:10,497] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,498] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,500] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,517] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,517] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,519] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,522] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,522] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:10,523] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:15,524] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:15,524] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:15,526] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:15,529] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:15,530] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:15,531] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:19,158] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:48:19,505] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 16:48:20,534] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:20,535] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:20,536] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:20,540] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:20,540] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:20,541] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:25,543] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:25,544] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:25,545] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:25,549] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:25,550] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:25,551] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:29,158] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:48:30,550] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:30,550] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:30,551] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:30,554] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:30,555] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:30,555] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:35,555] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:35,556] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:35,557] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:35,562] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:35,563] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:35,564] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:39,159] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:48:40,541] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,542] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,543] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,561] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,561] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,562] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,565] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,566] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:40,566] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:45,567] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:45,567] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:45,570] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:45,574] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:45,575] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:45,576] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:49,139] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:48:50,577] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:50,578] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:50,579] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:50,583] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:50,583] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:50,584] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:55,583] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:55,583] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:55,584] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:55,587] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:55,588] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:55,589] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:48:59,139] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:49:00,593] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:00,594] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:00,595] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:00,599] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:00,599] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:00,600] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:05,598] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:05,599] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:05,600] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:05,603] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:05,603] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:05,604] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:09,140] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:49:10,583] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,584] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,585] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,603] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,604] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,605] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,607] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,608] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:10,609] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:15,613] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:15,613] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:15,615] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:15,619] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:15,620] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:15,621] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:19,119] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:49:20,622] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:20,623] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:20,625] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:20,629] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:20,629] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:20,631] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:25,631] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:25,632] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:25,633] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:25,637] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:25,637] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:25,638] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:29,120] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:49:30,641] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:30,642] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:30,644] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:30,648] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:30,648] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:30,649] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:35,651] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:35,652] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:35,653] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:35,656] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:35,657] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:35,658] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:39,121] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:49:40,640] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,641] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,642] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,661] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,662] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,663] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,669] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,669] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:40,670] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:45,671] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:45,672] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:45,673] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:45,677] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:45,678] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:45,679] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:49,100] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:49:50,681] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:50,682] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:50,683] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:50,687] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:50,688] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:50,689] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:55,688] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:55,689] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:55,690] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:55,694] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:55,694] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:55,695] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:49:59,101] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:50:00,696] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:00,697] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:00,699] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:00,703] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:00,703] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:00,704] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:05,706] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:05,706] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:05,708] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:05,712] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:05,712] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:05,713] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:09,102] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:50:10,696] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,697] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,699] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,712] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,713] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,714] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,718] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,718] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:10,719] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:15,724] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:15,724] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:15,726] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:15,730] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:15,731] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:15,732] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:19,081] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:50:20,734] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:20,735] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:20,737] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:20,741] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:20,741] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:20,743] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:25,744] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:25,745] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:25,746] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:25,751] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:25,752] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:25,753] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:29,082] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:50:30,754] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:30,755] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:30,756] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:30,760] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:30,761] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:30,763] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:35,765] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:35,765] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:35,767] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:35,771] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:35,771] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:35,772] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:39,083] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:50:40,770] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:40,771] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:40,773] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:40,777] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:40,777] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:40,779] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:45,782] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:45,782] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:45,784] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:45,789] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:45,789] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:45,790] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:49,078] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:50:50,793] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:50,794] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:50,795] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:50,799] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:50,799] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:50,801] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:55,802] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:55,803] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:55,804] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:55,808] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:55,808] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:55,809] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:50:59,078] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:51:00,810] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:00,811] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:00,812] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:00,816] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:00,816] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:00,818] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:05,818] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:05,818] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:05,820] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:05,824] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:05,824] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:05,826] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:09,079] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:51:10,812] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,813] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,814] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,823] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,824] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,825] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,828] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,829] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:10,830] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:15,833] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:15,834] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:15,835] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:15,839] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:15,840] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:15,841] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:19,064] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:51:20,843] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:20,844] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:20,845] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:20,849] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:20,850] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:20,851] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:25,851] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:25,852] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:25,854] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:25,857] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:25,858] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:25,859] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:29,065] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:51:30,860] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:30,861] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:30,863] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:30,867] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:30,868] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:30,869] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:35,869] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:35,869] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:35,870] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:35,873] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:35,874] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:35,875] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:39,066] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:51:40,858] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,859] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,860] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,877] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,877] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,878] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,882] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,882] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:40,883] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:45,885] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:45,886] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:45,887] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:45,891] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:45,891] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:45,892] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:49,046] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:51:50,896] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:50,897] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:50,898] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:50,902] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:50,903] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:50,904] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:55,904] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:55,905] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:55,906] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:55,910] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:55,911] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:55,912] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:51:59,047] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:52:00,913] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:00,914] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:00,915] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:00,919] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:00,920] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:00,921] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:05,923] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:05,923] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:05,925] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:05,929] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:05,929] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:05,930] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:09,048] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:52:10,916] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,917] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,918] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,929] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,930] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,931] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,934] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,935] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:10,936] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:15,940] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:15,941] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:15,942] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:15,946] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:15,947] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:15,948] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:19,028] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:52:20,950] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:20,951] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:20,952] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:20,956] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:20,957] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:20,958] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:25,959] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:25,960] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:25,961] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:25,965] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:25,966] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:25,967] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:29,029] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:52:30,969] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:30,970] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:30,971] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:30,975] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:30,976] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:30,977] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:35,979] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:35,980] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:35,981] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:35,985] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:35,986] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:35,987] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:39,030] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:52:40,967] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,969] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,970] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,986] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,986] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,988] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,991] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,992] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:40,993] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:45,996] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:45,996] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:45,998] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:46,002] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:46,003] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:46,004] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:49,010] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:52:51,006] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:51,007] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:51,009] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:51,014] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:51,015] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:51,016] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:56,018] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:56,018] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:56,020] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:56,024] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:56,024] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:56,026] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:52:59,011] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:53:01,029] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:01,030] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:01,032] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:01,036] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:01,037] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:01,038] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:06,041] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:06,041] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:06,043] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:06,047] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:06,048] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:06,049] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:09,012] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:53:11,029] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,030] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,032] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,048] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,049] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,050] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,054] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,054] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:11,055] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:16,058] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:16,058] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:16,060] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:16,065] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:16,065] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:16,067] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:18,992] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:53:21,067] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:21,068] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:21,069] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:21,073] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:21,074] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:21,075] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:26,075] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:26,075] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:26,078] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:26,082] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:26,082] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:26,083] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:28,992] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:53:31,086] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:31,087] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:31,089] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:31,093] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:31,094] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:31,095] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:36,096] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:36,096] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:36,098] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:36,102] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:36,102] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:36,104] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:38,993] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:53:41,085] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,086] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,087] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,104] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,105] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,106] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,110] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,110] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:41,111] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:46,117] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:46,118] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:46,119] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:46,123] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:46,124] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:46,125] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:48,973] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:53:51,127] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:51,128] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:51,130] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:51,134] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:51,134] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:51,135] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:56,138] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:56,138] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:56,140] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:56,144] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:56,145] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:56,146] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:53:58,973] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:54:01,147] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:01,148] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:01,149] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:01,153] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:01,154] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:01,155] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:06,157] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:06,158] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:06,159] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:06,164] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:06,164] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:06,165] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:08,974] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:54:11,143] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,144] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,146] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,164] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,164] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,165] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,169] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,169] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:11,170] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:16,173] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:16,173] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:16,175] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:16,179] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:16,179] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:16,180] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:18,954] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:54:21,178] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:21,179] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:21,180] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:21,183] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:21,183] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:21,184] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:26,188] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:26,189] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:26,190] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:26,195] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:26,195] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:26,196] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:28,955] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:54:31,199] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:31,199] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:31,201] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:31,205] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:31,205] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:31,206] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:36,210] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:36,210] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:36,212] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:36,216] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:36,216] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:36,218] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:38,955] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:54:41,200] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,202] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,203] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,218] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,218] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,219] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,223] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,224] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:41,225] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:46,226] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:46,227] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:46,230] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:46,234] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:46,235] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:46,236] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:48,935] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:54:51,238] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:51,238] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:51,240] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:51,244] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:51,244] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:51,246] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:56,249] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:56,249] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:56,251] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:56,255] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:56,256] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:56,257] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:54:58,936] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:55:01,257] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:01,258] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:01,259] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:01,263] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:01,264] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:01,265] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:06,268] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:06,269] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:06,271] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:06,275] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:06,276] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:06,277] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:08,937] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:55:11,261] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,262] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,264] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,276] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,276] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,277] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,281] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,282] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:11,283] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:16,285] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:16,285] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:16,287] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:16,291] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:16,291] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:16,292] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:18,917] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:55:21,294] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:21,295] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:21,297] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:21,301] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:21,302] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:21,303] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:26,303] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:26,304] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:26,305] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:26,309] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:26,309] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:26,310] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:28,918] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:55:31,324] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:31,351] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:31,358] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:31,371] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:31,372] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:31,373] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:36,378] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:36,378] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:36,380] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:36,385] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:36,385] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:36,386] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:38,918] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:55:41,369] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,370] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,371] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,386] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,387] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,388] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,391] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,392] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:41,393] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:46,401] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:46,401] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:46,403] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:46,407] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:46,407] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:46,408] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:48,898] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:55:51,409] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:51,410] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:51,411] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:51,415] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:51,415] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:51,416] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:56,416] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:56,417] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:56,418] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:56,422] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:56,423] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:56,424] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:55:58,899] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:56:01,424] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:01,425] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:01,426] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:01,430] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:01,431] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:01,432] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:06,437] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:06,437] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:06,439] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:06,443] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:06,443] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:06,444] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:08,899] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:56:11,427] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,428] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,429] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,443] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,444] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,445] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,448] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,449] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:11,450] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:16,452] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:16,453] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:16,454] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:16,458] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:16,459] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:16,460] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:18,880] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:56:19,758] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 5 more
[2022-04-25 16:56:19,761] INFO Opened connection [connectionId{localValue:107, serverValue:18}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:56:19,761] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1005556} (org.mongodb.driver.cluster)
[2022-04-25 16:56:21,465] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:21,466] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:21,468] INFO Opened connection [connectionId{localValue:108, serverValue:19}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 16:56:21,469] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:21,473] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:21,473] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:21,474] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:26,476] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:26,476] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:26,477] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:26,482] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:26,482] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:26,483] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:28,880] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:56:31,485] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:31,486] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:31,487] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:31,492] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:31,492] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:31,493] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:36,494] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:36,494] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:36,496] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:36,501] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:36,501] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:36,502] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:38,881] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:56:41,481] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,482] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,483] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,502] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,503] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,504] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,507] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,508] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:41,509] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:46,513] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:46,514] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:46,516] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:46,520] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:46,521] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:46,522] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:48,861] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:56:51,522] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:51,524] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:51,526] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:51,537] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:51,537] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:51,538] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:56,534] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:56,535] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:56,536] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:56,540] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:56,540] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:56,542] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:56:58,862] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:57:01,544] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:01,545] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:01,547] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:01,551] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:01,552] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:01,553] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:06,555] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:06,555] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:06,557] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:06,561] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:06,562] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:06,563] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:08,863] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:57:11,547] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,548] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,550] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,562] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,563] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,564] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,568] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,568] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:11,569] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:16,572] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:16,572] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:16,574] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:16,579] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:16,580] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:16,581] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:18,843] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:57:21,582] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:21,583] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:21,584] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:21,588] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:21,589] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:21,590] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:26,593] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:26,594] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:26,595] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:26,600] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:26,600] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:26,609] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:28,844] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 16:57:31,603] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:31,604] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:31,605] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:31,610] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:31,610] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 16:57:31,611] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:34,010] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:27:34,396] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,013] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,330] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,011] INFO [Producer clientId=producer-3] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,290] INFO [Producer clientId=producer-1] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:33,723] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:33,513] INFO [Producer clientId=producer-2] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,415] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,374] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,370] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,358] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39321 due to node 2 being disconnected (elapsed time since creation: 1798837ms, elapsed time since send: 1798837ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,444] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39293 due to node 2 being disconnected (elapsed time since creation: 1799196ms, elapsed time since send: 1799196ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,537] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,537] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 13134 due to node 2 being disconnected (elapsed time since creation: 1798545ms, elapsed time since send: 1798545ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,539] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Error sending fetch request (sessionId=1799512198, epoch=13105) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:27:34,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39322 due to node 1 being disconnected (elapsed time since creation: 1798618ms, elapsed time since send: 1798618ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39323 due to node 3 being disconnected (elapsed time since creation: 1798473ms, elapsed time since send: 1798473ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,546] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,577] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,577] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,579] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1358878278, epoch=13087) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:27:34,587] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=2080934661, epoch=13087) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:27:34,587] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=297888554, epoch=13115) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:27:34,588] INFO [AdminClient clientId=adminclient-8] Cancelled in-flight METADATA request with correlation id 5986 due to node 1 being disconnected (elapsed time since creation: 247ms, elapsed time since send: 247ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,594] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39295 due to node 1 being disconnected (elapsed time since creation: 1798979ms, elapsed time since send: 1798979ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,595] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,595] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39294 due to node 3 being disconnected (elapsed time since creation: 1799157ms, elapsed time since send: 1799157ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:27:34,715] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=112260167, epoch=13087) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:27:34,715] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1798407499, epoch=13088) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:27:34,715] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=882543429, epoch=13086) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:27:34,730] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,731] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,787] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:27:34,794] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat with Generation{generationId=23, memberId='connect-1-4960f3b8-31a2-4f1f-9004-b3d9e4d6c511', protocol='sessioned'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,795] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,795] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,795] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,800] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,812] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=25, memberId='connect-1-45bd52d6-93fb-4ae9-bcc9-a317955c5f18', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:34,835] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:34,838] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:34,842] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:34,869] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:34,872] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:34,875] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:36,362] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=25, memberId='connect-1-45bd52d6-93fb-4ae9-bcc9-a317955c5f18', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:27:36,363] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 25 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-45bd52d6-93fb-4ae9-bcc9-a317955c5f18', leaderUrl='http://connect:8083/', offset=31, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:27:36,363] WARN [Worker clientId=connect-1, groupId=connect-cluster-group] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:27:36,363] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Current config state offset 30 is behind group assignment 31, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:27:36,368] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished reading to end of log and updated config snapshot, new config log offset: 31 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:27:36,368] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 31 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:27:36,368] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:27:39,845] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:39,846] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:39,847] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:39,850] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:39,851] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:39,851] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:43,584] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:27:44,865] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:44,869] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:44,874] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:44,904] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:44,908] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:44,913] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:49,937] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:49,939] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:49,945] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:49,981] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:49,982] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:49,986] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:53,586] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:27:54,992] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:54,999] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:55,005] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:55,036] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:55,041] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:27:55,049] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:00,033] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:00,038] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:00,045] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:00,062] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:00,063] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:00,067] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:03,634] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:28:05,110] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:05,113] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:05,117] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:05,133] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:05,135] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:05,139] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:10,132] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:10,134] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:10,138] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:10,164] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:10,165] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:10,169] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:13,636] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:28:15,154] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:15,156] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:15,160] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:15,177] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:15,178] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:15,184] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:20,176] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:20,178] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:20,182] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:20,196] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:20,198] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:28:20,201] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:28,399] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 5 more
[2022-04-25 17:43:28,128] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,428] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,754] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,754] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,350] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,350] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 13237 due to node 2 being disconnected (elapsed time since creation: 905840ms, elapsed time since send: 905840ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,350] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Error sending fetch request (sessionId=198769835, epoch=97) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:43:28,352] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,352] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,352] INFO [AdminClient clientId=adminclient-8] Cancelled in-flight METADATA request with correlation id 6155 due to node 3 being disconnected (elapsed time since creation: 4ms, elapsed time since send: 4ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,753] INFO [Producer clientId=producer-2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,486] INFO [Producer clientId=producer-1] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,486] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:43:28,486] INFO [Producer clientId=producer-3] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,360] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,360] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,360] INFO [Producer clientId=producer-3] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,482] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,360] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39617 due to node 1 being disconnected (elapsed time since creation: 905771ms, elapsed time since send: 905771ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,360] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,360] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39618 due to node 2 being disconnected (elapsed time since creation: 905740ms, elapsed time since send: 905740ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,361] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,361] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39619 due to node 3 being disconnected (elapsed time since creation: 905553ms, elapsed time since send: 905553ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,361] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1854948997, epoch=95) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:43:28,361] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=898269821, epoch=96) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:43:28,361] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1920591339, epoch=95) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:43:28,429] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,363] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39589 due to node 3 being disconnected (elapsed time since creation: 905380ms, elapsed time since send: 905380ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,363] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,363] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39591 due to node 1 being disconnected (elapsed time since creation: 905312ms, elapsed time since send: 905312ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,377] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,378] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39590 due to node 2 being disconnected (elapsed time since creation: 905343ms, elapsed time since send: 905343ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:28,439] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,440] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,441] INFO Opened connection [connectionId{localValue:109, serverValue:20}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 17:43:28,449] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat with Generation{generationId=25, memberId='connect-1-45bd52d6-93fb-4ae9-bcc9-a317955c5f18', protocol='sessioned'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,449] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,449] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,449] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,455] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,462] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19820437} (org.mongodb.driver.cluster)
[2022-04-25 17:43:28,555] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=27, memberId='connect-1-b6b4e57a-e62f-43d8-b01b-7dbabef63a53', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:28,555] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=436750008, epoch=96) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:43:28,556] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=376183522, epoch=95) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:43:28,556] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=330584080, epoch=95) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 17:43:28,682] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:28,684] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:28,715] INFO Opened connection [connectionId{localValue:110, serverValue:21}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 17:43:28,719] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:28,830] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:28,830] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:28,832] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:30,065] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] SyncGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=27, memberId='connect-1-b6b4e57a-e62f-43d8-b01b-7dbabef63a53', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,066] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,066] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,165] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:30,210] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,213] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,213] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,213] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,317] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,318] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,325] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,325] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,326] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=27, memberId='connect-1-b6b4e57a-e62f-43d8-b01b-7dbabef63a53', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,426] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:30,456] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka2:19092 (id: 2147483645 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,456] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,463] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] JoinGroup failed: The coordinator is not aware of this member. Need to re-join the group. Sent generation was Generation{generationId=27, memberId='connect-1-b6b4e57a-e62f-43d8-b01b-7dbabef63a53', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,463] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from JOIN_GROUP response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,463] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,464] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:30,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:33,729] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:33,730] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:33,731] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:33,735] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:33,735] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:33,736] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:34,982] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka2:19092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:34,982] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka2:19092 (id: 2147483645 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:34,982] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='connect-1-1143f988-7823-4254-a963-f43b5484fe3b', protocol='null'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:35,083] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483645 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 17:43:35,084] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:35,084] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:35,086] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] JoinGroup failed: The coordinator is not aware of this member. Need to re-join the group. Sent generation was Generation{generationId=-1, memberId='connect-1-1143f988-7823-4254-a963-f43b5484fe3b', protocol='null'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:35,086] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from JOIN_GROUP response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:35,086] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:35,086] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:35,087] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:38,360] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:43:38,756] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:38,758] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:38,762] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:38,779] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:38,781] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:38,785] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:43,778] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:43,780] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:43,783] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:43,797] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:43,799] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:43,805] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:45,059] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=26, memberId='connect-1-ae5db6bb-8025-4c23-a7a6-80e77218eb01', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:45,092] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=26, memberId='connect-1-ae5db6bb-8025-4c23-a7a6-80e77218eb01', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 17:43:45,092] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 26 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-ae5db6bb-8025-4c23-a7a6-80e77218eb01', leaderUrl='http://connect:8083/', offset=31, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:43:45,093] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 31 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:43:45,093] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 17:43:48,362] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:43:48,800] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:48,802] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:48,806] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:48,821] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:48,823] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:48,827] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:53,822] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:53,824] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:53,828] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:53,845] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:53,846] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:53,850] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,341] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:43:58,820] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,824] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,827] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,846] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,847] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,851] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,872] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,873] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:43:58,877] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:03,906] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:03,911] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:03,918] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:03,958] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:03,961] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:03,965] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:08,342] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 17:44:08,941] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:08,944] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:08,948] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:08,973] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:08,974] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:08,978] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:13,971] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:13,973] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:13,977] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:13,992] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:13,994] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 17:44:13,997] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:54,516] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,092] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,536] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:01:54,535] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,535] INFO [Producer clientId=producer-3] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,096] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,535] INFO [Producer clientId=producer-2] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,527] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,096] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 13336 due to node 2 being disconnected (elapsed time since creation: 1057191ms, elapsed time since send: 1057191ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,097] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39914 due to node 1 being disconnected (elapsed time since creation: 1057898ms, elapsed time since send: 1057897ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,097] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Error sending fetch request (sessionId=1704684649, epoch=95) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:01:54,098] INFO [Producer clientId=producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,093] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,097] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39915 due to node 2 being disconnected (elapsed time since creation: 1057857ms, elapsed time since send: 1057857ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,100] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39916 due to node 3 being disconnected (elapsed time since creation: 1057418ms, elapsed time since send: 1057418ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=354790389, epoch=96) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:01:54,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=856773227, epoch=95) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:01:54,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1285971935, epoch=96) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:01:54,527] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,102] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,092] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,107] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39886 due to node 2 being disconnected (elapsed time since creation: 1057422ms, elapsed time since send: 1057422ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,107] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,107] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39887 due to node 1 being disconnected (elapsed time since creation: 1057354ms, elapsed time since send: 1057354ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,107] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,107] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 39888 due to node 3 being disconnected (elapsed time since creation: 1056983ms, elapsed time since send: 1056983ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:54,178] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,184] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,193] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat with Generation{generationId=26, memberId='connect-1-ae5db6bb-8025-4c23-a7a6-80e77218eb01', protocol='sessioned'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,193] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,193] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,193] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,198] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,263] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=28, memberId='connect-1-6c024c18-8028-41c8-b1fd-ae7982d13d9a', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:54,579] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1631367028, epoch=95) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:01:54,580] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1214346569, epoch=95) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:01:54,580] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1874689065, epoch=97) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:01:54,589] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:54,590] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:54,593] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:54,646] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:54,647] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:54,649] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:55,779] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=28, memberId='connect-1-6c024c18-8028-41c8-b1fd-ae7982d13d9a', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:55,780] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 28 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6c024c18-8028-41c8-b1fd-ae7982d13d9a', leaderUrl='http://connect:8083/', offset=31, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:01:55,780] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 31 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:01:55,780] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:01:56,171] WARN [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] 8 partitions have leader brokers without a matching listener, including [docker-connect-offsets-5, docker-connect-offsets-20, docker-connect-offsets-11, docker-connect-offsets-17, docker-connect-offsets-23, docker-connect-offsets-2, docker-connect-offsets-8, docker-connect-offsets-14] (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:56,175] WARN [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] 1 partitions have leader brokers without a matching listener, including [docker-connect-status-2] (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:57,264] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat failed since coordinator kafka1:19091 (id: 2147483646 rack: null) is either not started or not valid (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:57,264] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:57,264] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:57,264] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:01:57,265] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka2:19092 (id: 2147483645 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:01:59,599] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:59,599] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:59,601] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:59,606] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:59,606] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:01:59,608] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:03,367] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat failed since coordinator kafka2:19092 (id: 2147483645 rack: null) is either not started or not valid (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:02:03,367] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka2:19092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:02:03,367] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka2:19092 (id: 2147483645 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:02:03,367] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483645 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:02:03,368] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:02:04,097] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:02:04,617] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:04,618] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:04,622] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:04,645] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:04,646] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:04,653] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:09,639] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:09,641] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:09,645] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:09,660] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:09,661] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:09,665] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:14,099] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:02:14,661] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:14,663] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:14,667] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:14,682] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:14,687] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:14,690] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:19,699] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:19,700] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:19,704] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:19,718] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:19,719] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:19,723] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:24,075] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:02:24,742] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:24,745] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:24,748] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:24,762] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:24,764] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:24,767] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:29,769] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:29,770] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:29,774] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:29,803] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:29,804] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:29,808] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:34,077] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:02:34,790] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:34,793] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:34,796] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:34,852] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:34,854] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:34,857] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:39,861] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:39,862] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:39,866] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:39,885] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:39,887] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:02:39,890] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:30,520] INFO [Producer clientId=producer-2] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,525] INFO [Producer clientId=producer-3] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,527] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,527] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,527] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,530] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 5 more
[2022-04-25 18:20:30,536] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:20:30,520] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,537] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,537] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,537] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40260 due to node 1 being disconnected (elapsed time since creation: 1067648ms, elapsed time since send: 1067648ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,538] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,538] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40262 due to node 2 being disconnected (elapsed time since creation: 1067339ms, elapsed time since send: 1067339ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,538] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,538] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40261 due to node 3 being disconnected (elapsed time since creation: 1067625ms, elapsed time since send: 1067625ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,543] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,544] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,544] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 13437 due to node 2 being disconnected (elapsed time since creation: 1067340ms, elapsed time since send: 1067340ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,544] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Error sending fetch request (sessionId=1963846580, epoch=97) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40286 due to node 1 being disconnected (elapsed time since creation: 1067709ms, elapsed time since send: 1067709ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40288 due to node 2 being disconnected (elapsed time since creation: 1067345ms, elapsed time since send: 1067345ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40287 due to node 3 being disconnected (elapsed time since creation: 1067630ms, elapsed time since send: 1067630ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=2003268917, epoch=169) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=2105562061, epoch=96) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:20:30,545] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1953908890, epoch=96) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:20:30,587] INFO [Producer clientId=producer-1] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,589] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:30,258] INFO Opened connection [connectionId{localValue:111, serverValue:22}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 18:20:30,258] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2856334} (org.mongodb.driver.cluster)
[2022-04-25 18:20:30,447] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,458] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,458] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,458] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,562] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,563] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,563] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,637] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=74071524, epoch=171) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:20:30,637] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1310574947, epoch=96) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:20:30,637] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=135034442, epoch=96) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:20:30,674] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:30,699] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,699] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:30,736] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat with Generation{generationId=28, memberId='connect-1-6c024c18-8028-41c8-b1fd-ae7982d13d9a', protocol='sessioned'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,736] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,736] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,736] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,736] INFO Opened connection [connectionId{localValue:112, serverValue:23}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 18:20:30,738] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:30,742] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:30,914] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:30,916] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:30,917] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:30,969] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-7adceefc-866b-4dd7-aeb2-c33816a0daae', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:31,995] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-7adceefc-866b-4dd7-aeb2-c33816a0daae', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:31,995] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-7adceefc-866b-4dd7-aeb2-c33816a0daae', leaderUrl='http://connect:8083/', offset=31, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:20:31,996] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 31 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:20:31,996] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:20:33,970] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat failed since coordinator kafka1:19091 (id: 2147483646 rack: null) is either not started or not valid (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:33,971] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:33,971] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:33,971] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:33,972] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka2:19092 (id: 2147483645 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:35,743] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:35,744] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:35,745] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:35,749] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:35,749] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:35,750] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:39,886] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:20:40,080] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat failed since coordinator kafka2:19092 (id: 2147483645 rack: null) is either not started or not valid (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:40,081] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka2:19092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:40,081] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka2:19092 (id: 2147483645 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:40,082] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483645 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:20:40,085] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:20:40,774] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:40,776] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:40,779] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:40,794] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:40,796] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:40,799] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:45,795] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:45,796] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:45,800] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:45,824] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:45,825] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:45,829] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:49,887] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:20:50,818] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:50,820] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:50,824] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:50,841] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:50,842] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:50,846] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:55,842] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:55,843] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:55,847] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:55,864] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:55,866] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:55,898] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:20:59,889] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:21:00,623] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,626] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,629] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,863] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,864] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,868] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,882] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,883] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:00,886] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:05,883] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:05,885] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:05,889] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:05,915] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:05,917] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:05,921] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:09,651] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:21:10,919] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:10,922] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:10,925] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:10,939] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:10,941] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:10,944] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:15,938] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:15,939] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:15,943] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:15,956] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:15,958] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:21:15,962] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:48,394] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,912] INFO [Producer clientId=producer-1] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:48,407] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,913] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40584 due to node 3 being disconnected (elapsed time since creation: 989460ms, elapsed time since send: 989460ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,913] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,913] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40585 due to node 1 being disconnected (elapsed time since creation: 989308ms, elapsed time since send: 989308ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,913] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,913] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40586 due to node 2 being disconnected (elapsed time since creation: 989165ms, elapsed time since send: 989165ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,914] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=295350633, epoch=97) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:37:47,914] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1031238141, epoch=95) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:37:47,914] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=670216534, epoch=96) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:37:48,407] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,914] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40558 due to node 3 being disconnected (elapsed time since creation: 989459ms, elapsed time since send: 989459ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,912] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:47,914] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,914] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40559 due to node 1 being disconnected (elapsed time since creation: 989309ms, elapsed time since send: 989309ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,914] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,915] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40560 due to node 2 being disconnected (elapsed time since creation: 989295ms, elapsed time since send: 989295ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,917] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,917] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,917] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,917] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 13537 due to node 2 being disconnected (elapsed time since creation: 988807ms, elapsed time since send: 988807ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,918] INFO [Producer clientId=producer-3] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,918] INFO [Producer clientId=producer-2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,918] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 5 more
[2022-04-25 18:37:47,919] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Error sending fetch request (sessionId=1973653521, epoch=96) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:37:47,923] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,923] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:37:47,925] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:47,926] INFO Opened connection [connectionId{localValue:113, serverValue:24}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 18:37:47,926] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2281453} (org.mongodb.driver.cluster)
[2022-04-25 18:37:48,089] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,089] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=2045362025, epoch=95) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:37:48,089] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,090] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,090] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1870203989, epoch=97) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:37:48,090] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=477416092, epoch=96) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 18:37:48,181] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:48,183] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:48,186] INFO Opened connection [connectionId{localValue:114, serverValue:25}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 18:37:48,188] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:48,213] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,213] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,213] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,221] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:48,222] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:48,224] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:48,342] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,342] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,342] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,447] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,459] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:37:48,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat with Generation{generationId=1, memberId='connect-1-7adceefc-866b-4dd7-aeb2-c33816a0daae', protocol='sessioned'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,466] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,467] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:48,470] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-6afbe371-5d2e-4111-802b-1cee203ec1fb', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:49,310] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-6afbe371-5d2e-4111-802b-1cee203ec1fb', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 18:37:49,310] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6afbe371-5d2e-4111-802b-1cee203ec1fb', leaderUrl='http://connect:8083/', offset=32, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:37:49,310] WARN [Worker clientId=connect-1, groupId=connect-cluster-group] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:37:49,310] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Current config state offset 31 is behind group assignment 32, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:37:49,314] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished reading to end of log and updated config snapshot, new config log offset: 32 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:37:49,314] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 32 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:37:49,314] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 18:37:49,469] WARN [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] 1 partitions have leader brokers without a matching listener, including [docker-connect-configs-0] (org.apache.kafka.clients.NetworkClient)
[2022-04-25 18:37:53,195] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:53,195] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:53,197] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:53,202] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:53,202] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:53,204] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:57,924] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:37:58,216] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:58,219] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:58,223] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:58,243] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:58,248] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:37:58,258] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:03,240] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:03,242] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:03,246] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:03,262] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:03,264] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:03,268] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:07,926] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:38:08,264] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:08,267] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:08,271] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:08,284] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:08,286] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:08,289] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:13,285] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:13,287] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:13,290] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:13,306] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:13,308] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:13,311] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:18,082] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:38:18,459] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:18,461] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:18,465] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:18,490] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:18,492] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:18,496] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:23,524] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:23,527] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:23,531] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:23,546] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:23,548] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:23,551] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:28,083] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 18:38:28,546] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:28,550] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:28,553] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:28,570] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:28,572] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:28,576] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:33,570] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:33,572] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:33,576] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:33,592] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:33,595] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 18:38:33,599] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:26,962] INFO [Producer clientId=producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:26,972] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,050] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:23,671] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,057] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40886 due to node 3 being disconnected (elapsed time since creation: 4486088ms, elapsed time since send: 4486088ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,058] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,058] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40888 due to node 1 being disconnected (elapsed time since creation: 4485887ms, elapsed time since send: 4485887ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,058] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,058] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40887 due to node 2 being disconnected (elapsed time since creation: 4485918ms, elapsed time since send: 4485918ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,058] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1790331970, epoch=97) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 19:53:23,671] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:23,671] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 5 more
[2022-04-25 19:53:23,670] INFO [Producer clientId=producer-2] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:23,670] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:23,670] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:23,670] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:53:24,064] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,064] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40860 due to node 3 being disconnected (elapsed time since creation: 4486087ms, elapsed time since send: 4486087ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,064] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:23,668] INFO [Producer clientId=producer-3] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,064] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40862 due to node 1 being disconnected (elapsed time since creation: 4485721ms, elapsed time since send: 4485721ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,064] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,064] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 40861 due to node 2 being disconnected (elapsed time since creation: 4485917ms, elapsed time since send: 4485917ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:26,977] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,064] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 13799 due to node 2 being disconnected (elapsed time since creation: 4485917ms, elapsed time since send: 4485917ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,065] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Error sending fetch request (sessionId=576540060, epoch=254) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 19:53:24,099] INFO [Producer clientId=producer-3] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,099] INFO [Producer clientId=producer-3] Cancelled in-flight METADATA request with correlation id 74 due to node 2 being disconnected (elapsed time since creation: 2ms, elapsed time since send: 2ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=508845497, epoch=97) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 19:53:24,100] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=284716824, epoch=98) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 19:53:24,102] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:24,113] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,139] INFO Opened connection [connectionId{localValue:115, serverValue:26}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 19:53:24,139] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=29005280} (org.mongodb.driver.cluster)
[2022-04-25 19:53:24,167] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,178] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,413] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 19:53:24,420] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat with Generation{generationId=1, memberId='connect-1-6afbe371-5d2e-4111-802b-1cee203ec1fb', protocol='sessioned'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,421] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,421] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,421] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,448] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:24,535] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=3, memberId='connect-1-c318ccdc-879f-48b1-b120-d48fd3671709', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:26,724] WARN [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] 1 partitions have leader brokers without a matching listener, including [docker-connect-status-2] (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:27,209] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1119966454, epoch=97) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 19:53:27,209] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=587787376, epoch=97) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 19:53:27,209] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1951417834, epoch=98) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 19:53:27,222] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] SyncGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=3, memberId='connect-1-c318ccdc-879f-48b1-b120-d48fd3671709', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,222] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,222] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,247] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:27,248] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:27,251] INFO Opened connection [connectionId{localValue:116, serverValue:27}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 19:53:27,251] WARN [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] 8 partitions have leader brokers without a matching listener, including [docker-connect-offsets-5, docker-connect-offsets-20, docker-connect-offsets-11, docker-connect-offsets-17, docker-connect-offsets-23, docker-connect-offsets-2, docker-connect-offsets-8, docker-connect-offsets-14] (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:27,254] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:27,323] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:27,324] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka3:19093 (id: 2147483644 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,326] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka3:19093 (id: 2147483644 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,326] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,340] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] JoinGroup failed: The coordinator is not aware of this member. Need to re-join the group. Sent generation was Generation{generationId=3, memberId='connect-1-c318ccdc-879f-48b1-b120-d48fd3671709', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,340] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from JOIN_GROUP response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,340] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,340] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,395] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:27,785] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:27,786] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:27,788] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:30,560] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka3:19093 (id: 2147483644 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,561] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Requesting disconnect from last known coordinator kafka3:19093 (id: 2147483644 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,561] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='connect-1-516f68ec-b7f8-4338-84dd-0785d44d5e97', protocol='null'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,661] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Client requested disconnect from node 2147483644 (org.apache.kafka.clients.NetworkClient)
[2022-04-25 19:53:30,663] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,663] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,665] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] JoinGroup failed: The coordinator is not aware of this member. Need to re-join the group. Sent generation was Generation{generationId=-1, memberId='connect-1-516f68ec-b7f8-4338-84dd-0785d44d5e97', protocol='null'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,665] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from JOIN_GROUP response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,665] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,665] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:30,666] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:32,266] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:32,267] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:32,269] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:32,284] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:32,285] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:32,286] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:34,067] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:53:37,289] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:37,295] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:37,300] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:37,320] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:37,322] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:37,326] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:40,622] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=2, memberId='connect-1-35bb9dd0-3e59-40f9-b149-9bbb7086542d', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:40,662] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=2, memberId='connect-1-35bb9dd0-3e59-40f9-b149-9bbb7086542d', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 19:53:40,662] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-35bb9dd0-3e59-40f9-b149-9bbb7086542d', leaderUrl='http://connect:8083/', offset=33, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 19:53:40,663] WARN [Worker clientId=connect-1, groupId=connect-cluster-group] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 19:53:40,663] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Current config state offset 32 is behind group assignment 33, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 19:53:40,670] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished reading to end of log and updated config snapshot, new config log offset: 33 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 19:53:40,670] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 33 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 19:53:40,670] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 19:53:42,342] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:42,345] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:42,349] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:42,367] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:42,368] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:42,372] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:44,096] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:53:47,368] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:47,371] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:47,374] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:47,389] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:47,390] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:47,396] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:52,440] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:52,442] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:52,445] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:52,466] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:52,467] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:52,471] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:54,079] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:53:57,452] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:57,457] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:57,461] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:57,480] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:57,483] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:53:57,488] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:02,505] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:02,508] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:02,511] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:02,534] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:02,535] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:02,540] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:04,085] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:54:07,559] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:07,564] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:07,572] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:07,603] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:07,605] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:07,611] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:12,589] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:12,591] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:12,596] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:12,623] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:12,625] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:12,629] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:14,088] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:54:24,171] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:54:28,846] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:29,145] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:29,385] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:31,323] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:31,333] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:31,361] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:34,257] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:54:34,686] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:34,690] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:34,698] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:34,738] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:34,739] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:34,744] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:39,788] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:39,791] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:39,796] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:39,893] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:39,896] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:39,902] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:44,263] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:54:44,941] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:44,946] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:44,952] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:44,985] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:44,988] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:44,995] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:50,023] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:50,026] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:50,030] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:50,076] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:50,078] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:50,082] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:54,234] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:54:55,194] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:55,197] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:55,201] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:55,283] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:55,286] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:54:55,290] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:00,245] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:00,247] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:00,251] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:00,266] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:00,268] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:00,272] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:04,250] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:55:05,267] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:05,270] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:05,273] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:05,287] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:05,289] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:05,292] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:10,300] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:10,302] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:10,305] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:10,320] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:10,321] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:10,325] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:14,252] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:55:15,320] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:15,323] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:15,327] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:15,341] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:15,343] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:15,347] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:20,368] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:20,369] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:20,373] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:20,389] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:20,391] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:20,395] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:24,233] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:55:25,373] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:25,377] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:25,382] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:25,407] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:25,409] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:25,415] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:30,401] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:30,403] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:30,407] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:30,436] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:30,439] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:30,444] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:34,243] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:55:35,452] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:35,454] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:35,458] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:35,476] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:35,477] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:35,481] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:40,504] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:40,505] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:40,509] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:40,548] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:40,550] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:40,553] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:44,252] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:55:45,525] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:45,541] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:45,546] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:45,565] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:45,567] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:45,571] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:50,578] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:50,580] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:50,584] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:50,604] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:50,606] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:50,609] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:54,232] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:55:55,580] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:55,583] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:55,587] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:55,607] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:55,609] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:55:55,613] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:00,615] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:00,617] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:00,620] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:00,639] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:00,641] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:00,645] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:04,234] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:56:05,650] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:05,653] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:05,657] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:05,672] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:05,674] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:05,678] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:10,672] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:10,674] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:10,678] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:10,691] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:10,693] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:10,697] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:14,236] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:56:15,710] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:15,712] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:15,727] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:15,743] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:15,745] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:15,749] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:20,804] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:20,805] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:20,809] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:20,828] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:20,829] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:20,833] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:24,216] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 19:56:25,814] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:25,817] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:25,821] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:25,838] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:25,840] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 19:56:25,843] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:24,614] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:25,001] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,968] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,968] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:22:24,652] INFO [Producer clientId=producer-1] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,651] INFO [Producer clientId=producer-2] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,651] INFO [Producer clientId=producer-3] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,261] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,265] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,627] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 5 more
[2022-04-25 20:22:24,326] INFO Opened connection [connectionId{localValue:117, serverValue:28}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 20:22:24,326] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1757239} (org.mongodb.driver.cluster)
[2022-04-25 20:22:24,614] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,332] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 41842 due to node 2 being disconnected (elapsed time since creation: 1558257ms, elapsed time since send: 1558257ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,332] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,332] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 41844 due to node 1 being disconnected (elapsed time since creation: 1558213ms, elapsed time since send: 1558213ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,332] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,332] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 41843 due to node 3 being disconnected (elapsed time since creation: 1558253ms, elapsed time since send: 1558253ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,618] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,618] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,273] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 14145 due to node 2 being disconnected (elapsed time since creation: 1558126ms, elapsed time since send: 1558126ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,336] INFO [Consumer clientId=consumer-connect-cluster-group-3, groupId=connect-cluster-group] Error sending fetch request (sessionId=2130330705, epoch=340) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 20:22:24,337] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,338] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Group coordinator kafka1:19091 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,339] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 41904 due to node 3 being disconnected (elapsed time since creation: 1558240ms, elapsed time since send: 1558240ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,339] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 1 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,339] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 41905 due to node 1 being disconnected (elapsed time since creation: 1558225ms, elapsed time since send: 1558225ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,339] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,340] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Cancelled in-flight FETCH request with correlation id 41906 due to node 2 being disconnected (elapsed time since creation: 1558130ms, elapsed time since send: 1558130ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:24,340] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1196951923, epoch=334) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 20:22:24,340] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1321797839, epoch=336) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 20:22:24,340] INFO [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] Error sending fetch request (sessionId=1337416882, epoch=338) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 20:22:24,511] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,521] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Discovered group coordinator kafka1:19091 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,528] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Attempt to heartbeat with Generation{generationId=2, memberId='connect-1-35bb9dd0-3e59-40f9-b149-9bbb7086542d', protocol='sessioned'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,528] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Resetting generation due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,528] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,528] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,532] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:24,568] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=4, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:25,073] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1947947171, epoch=323) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 20:22:25,074] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1097539979, epoch=325) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 20:22:25,075] INFO [Consumer clientId=consumer-connect-cluster-group-1, groupId=connect-cluster-group] Error sending fetch request (sessionId=1568573192, epoch=323) to node 3: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2022-04-25 20:22:25,083] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:25,086] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:25,095] INFO Opened connection [connectionId{localValue:118, serverValue:29}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 20:22:25,103] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:25,128] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:25,129] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:25,131] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:25,630] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=4, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 20:22:25,630] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', leaderUrl='http://connect:8083/', offset=33, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 20:22:25,631] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 33 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 20:22:25,631] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 20:22:26,447] WARN [Consumer clientId=consumer-connect-cluster-group-2, groupId=connect-cluster-group] 4 partitions have leader brokers without a matching listener, including [docker-connect-status-0, docker-connect-status-4, docker-connect-status-1, docker-connect-status-3] (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:22:30,108] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:30,109] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:30,112] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:30,115] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:30,115] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:30,116] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:34,252] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:22:35,118] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:35,119] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:35,120] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:35,124] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:35,124] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:35,125] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:40,125] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:40,125] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:40,126] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:40,130] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:40,130] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:40,131] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:44,253] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:22:45,131] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:45,131] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:45,133] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:45,136] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:45,136] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:45,137] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:50,137] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:50,137] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:50,138] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:50,143] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:50,143] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:50,144] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:54,435] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:22:55,323] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:55,324] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:55,325] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:55,341] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:55,342] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:22:55,343] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:00,330] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:00,330] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:00,331] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:00,335] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:00,335] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:00,336] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:04,436] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:23:05,337] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:05,338] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:05,339] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:05,342] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:05,342] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:05,343] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:10,344] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:10,344] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:10,345] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:10,348] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:10,349] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:10,350] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:14,438] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:23:15,371] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:15,372] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:15,373] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:15,376] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:15,376] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:15,377] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:20,377] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:20,378] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:20,379] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:20,382] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:20,382] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:20,383] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:24,418] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:23:25,364] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,366] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,367] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,382] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,382] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,384] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,388] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,388] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:25,390] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:30,390] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:30,390] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:30,392] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:30,396] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:30,396] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:30,399] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:34,419] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:23:35,396] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:35,397] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:35,399] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:35,403] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:35,403] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:35,404] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:40,404] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:40,404] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:40,406] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:40,409] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:40,410] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:40,411] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:44,420] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:23:45,409] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:45,410] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:45,411] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:45,414] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:45,414] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:45,417] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:50,417] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:50,418] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:50,419] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:50,424] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:50,424] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:50,426] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:54,399] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:23:55,402] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,403] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,404] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,423] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,423] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,424] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,430] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,431] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:23:55,432] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:00,428] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:00,429] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:00,430] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:00,435] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:00,436] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:00,437] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:04,401] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:24:05,434] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:05,436] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:05,438] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:05,442] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:05,443] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:05,444] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:10,443] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:10,443] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:10,445] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:10,448] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:10,448] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:10,449] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:14,402] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:24:15,449] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:15,449] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:15,451] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:15,454] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:15,455] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:15,456] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:20,455] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:20,455] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:20,457] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:20,460] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:20,460] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:20,461] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:24,384] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:24:25,441] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,443] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,444] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,461] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,462] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,463] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,468] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,469] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:25,470] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:30,468] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:30,469] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:30,470] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:30,473] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:30,474] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:30,475] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:34,385] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:24:35,474] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:35,475] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:35,476] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:35,480] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:35,480] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:35,481] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:40,480] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:40,480] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:40,482] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:40,485] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:40,485] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:40,486] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:44,387] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:24:45,487] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:45,489] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:45,490] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:45,494] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:45,495] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:45,495] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:50,495] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:50,495] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:50,496] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:50,499] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:50,500] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:50,501] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:54,366] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:24:55,477] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,478] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,480] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,502] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,503] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,504] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,508] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,509] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:24:55,510] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:00,510] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:00,511] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:00,512] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:00,517] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:00,517] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:00,518] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:04,368] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:25:05,517] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:05,518] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:05,519] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:05,524] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:05,525] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:05,526] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:10,528] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:10,529] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:10,531] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:10,534] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:10,534] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:10,536] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:14,369] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:25:15,534] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:15,543] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:15,545] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:15,550] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:15,551] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:15,552] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:20,550] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:20,551] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:20,554] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:20,557] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:20,557] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:20,558] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:24,349] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:25:25,538] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,539] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,540] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,558] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,558] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,560] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,565] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,565] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:25,566] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:30,566] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:30,569] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:30,571] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:30,577] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:30,579] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:30,580] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:34,352] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:25:35,576] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:35,577] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:35,578] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:35,582] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:35,582] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:35,583] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:40,582] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:40,582] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:40,584] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:40,588] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:40,588] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:40,590] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:44,353] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:25:45,588] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:45,589] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:45,590] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:45,600] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:45,601] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:45,602] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:50,594] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:50,594] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:50,596] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:50,600] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:50,600] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:50,601] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:54,334] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:25:55,579] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,580] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,582] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,599] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,600] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,601] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,606] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,607] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:25:55,608] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:00,605] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:00,606] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:00,608] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:00,611] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:00,611] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:00,613] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:04,335] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:26:05,612] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:05,612] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:05,614] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:05,618] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:05,619] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:05,620] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:10,619] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:10,619] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:10,621] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:10,662] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:10,663] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:10,664] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:14,336] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:26:15,627] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:15,627] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:15,629] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:15,633] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:15,633] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:15,635] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:20,634] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:20,635] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:20,636] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:20,640] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:20,641] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:20,643] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:24,316] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:26:25,619] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,620] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,621] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,640] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,640] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,641] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,645] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,645] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:25,647] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:30,669] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:30,669] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:30,671] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:30,676] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:30,676] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:30,678] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:34,318] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:26:35,675] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:35,676] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:35,677] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:35,682] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:35,682] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:35,685] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:40,681] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:40,682] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:40,683] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:40,687] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:40,688] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:40,689] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:44,320] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:26:45,687] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:45,688] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:45,689] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:45,694] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:45,694] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:45,695] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:50,693] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:50,693] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:50,695] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:50,698] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:50,699] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:50,700] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:54,300] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:26:55,677] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,678] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,680] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,699] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,700] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,701] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,714] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,714] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:26:55,715] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:00,705] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:00,706] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:00,708] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:00,729] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:00,729] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:00,733] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:04,301] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:27:05,712] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:05,713] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:05,715] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:05,720] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:05,720] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:05,721] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:10,719] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:10,720] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:10,722] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:10,725] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:10,726] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:10,727] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:14,304] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:27:15,725] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:15,726] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:15,727] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:15,731] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:15,731] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:15,732] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:20,733] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:20,734] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:20,735] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:20,739] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:20,739] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:20,740] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:24,284] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:27:25,718] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,719] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,721] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,738] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,739] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,740] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,744] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,744] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:25,745] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:30,746] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:30,746] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:30,748] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:30,753] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:30,754] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:30,755] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:34,288] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:27:35,752] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:35,753] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:35,754] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:35,758] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:35,758] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:35,760] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:40,770] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:40,770] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:40,771] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:40,775] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:40,775] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:40,776] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:44,291] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:27:45,776] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:45,777] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:45,779] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:45,782] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:45,783] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:45,784] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:50,783] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:50,783] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:50,784] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:50,787] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:50,788] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:50,789] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:54,271] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:27:55,771] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,772] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,774] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,788] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,789] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,790] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,793] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,794] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:27:55,795] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:00,793] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:00,794] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:00,796] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:00,800] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:00,800] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:00,801] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:04,272] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:28:05,800] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:05,801] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:05,803] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:05,807] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:05,807] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:05,809] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:10,808] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:10,809] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:10,810] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:10,813] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:10,813] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:10,814] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:14,274] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:28:15,817] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:15,818] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:15,819] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:15,822] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:15,823] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:15,823] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:20,823] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:20,823] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:20,824] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:20,827] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:20,828] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:20,829] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:24,253] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:28:25,809] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,810] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,812] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,828] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,828] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,829] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,833] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,833] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:25,834] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:30,833] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:30,834] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:30,835] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:30,843] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:30,843] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:30,845] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:34,255] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:28:35,838] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:35,839] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:35,841] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:35,845] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:35,846] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:35,847] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:40,845] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:40,845] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:40,847] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:40,850] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:40,850] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:40,851] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:44,257] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:28:45,851] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:45,852] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:45,854] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:45,857] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:45,857] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:45,858] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:50,869] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:50,870] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:50,871] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:50,878] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:50,879] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:50,880] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:54,237] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:28:55,853] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,854] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,855] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,875] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,875] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,876] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,884] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,884] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:28:55,885] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:00,881] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:00,881] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:00,883] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:00,886] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:00,887] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:00,889] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:04,238] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:29:05,886] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:05,887] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:05,888] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:05,891] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:05,891] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:05,893] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:10,892] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:10,893] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:10,894] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:10,903] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:10,904] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:10,905] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:14,239] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:29:15,906] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:15,907] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:15,908] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:15,912] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:15,912] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:15,914] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:20,912] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:20,913] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:20,914] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:20,918] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:20,918] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:20,920] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:24,219] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:29:25,900] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,902] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,903] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,921] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,921] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,923] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,926] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,926] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:25,927] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:30,928] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:30,928] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:30,930] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:30,934] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:30,934] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:30,935] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:34,220] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:29:35,933] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:35,933] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:35,934] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:35,937] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:35,937] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:35,938] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:40,946] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:40,947] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:40,948] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:40,952] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:40,953] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:40,954] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:44,221] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:29:45,954] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:45,955] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:45,956] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:45,960] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:45,960] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:45,961] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:50,965] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:50,966] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:50,967] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:50,971] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:50,972] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:50,973] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:54,200] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:29:55,952] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,953] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,954] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,975] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,975] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,976] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,981] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,981] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:29:55,982] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:00,981] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:00,981] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:00,983] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:00,989] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:00,989] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:00,990] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:04,202] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:30:05,987] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:05,988] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:05,989] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:05,992] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:05,992] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:05,993] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:10,994] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:10,994] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:10,995] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:10,999] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:10,999] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:11,001] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:14,202] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:30:16,002] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:16,003] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:16,004] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:16,009] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:16,010] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:16,011] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:21,009] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:21,009] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:21,011] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:21,015] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:21,015] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:21,016] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:24,183] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:30:26,000] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,001] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,002] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,016] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,016] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,017] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,021] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,021] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:26,022] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:31,023] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:31,024] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:31,025] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:31,029] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:31,029] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:31,030] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:34,184] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:30:36,034] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:36,035] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:36,036] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:36,041] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:36,041] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:36,042] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:41,041] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:41,041] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:41,042] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:41,046] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:41,046] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:41,047] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:44,185] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:30:46,050] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:46,051] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:46,053] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:46,057] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:46,057] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:46,058] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:51,062] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:51,062] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:51,064] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:51,067] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:51,068] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:51,069] WARN Failed to resume change stream: The $changeStream stage is only supported on replica sets 40573

=====================================================================================
If the resume token is no longer available then there is the potential for data loss.
Saved resume tokens are managed by Kafka and stored with the offset data.

To restart the change stream with no resume token either: 
  * Create a new partition name using the `offset.partition.name` configuration.
  * Set `errors.tolerance=all` and ignore the erroring resume token. 
  * Manually remove the old offset from its configured storage.

Resetting the offset will allow for the connector to be resume from the latest resume
token. Using `copy.existing=true` ensures that all data will be outputted by the
connector but it will duplicate existing data.
=====================================================================================
 (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:51,647] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:355)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:223)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 20:30:54,165] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:30:56,048] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:56,049] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:30:56,049] INFO Cluster description not yet available. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster)
[2022-04-25 20:31:04,167] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:31:06,056] INFO Exception in monitor thread while connecting to server host.docker.internal:27017 (org.mongodb.driver.cluster)
com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message
	at com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:671)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:555)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:395)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:319)
	at com.mongodb.internal.connection.CommandHelper.sendAndReceive(CommandHelper.java:88)
	at com.mongodb.internal.connection.CommandHelper.executeCommand(CommandHelper.java:36)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:129)
	at com.mongodb.internal.connection.InternalStreamConnectionInitializer.startHandshake(InternalStreamConnectionInitializer.java:71)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:167)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:195)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:151)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)
	at com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:688)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:552)
	... 10 more
[2022-04-25 20:31:14,168] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:31:24,147] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:31:26,035] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:31:26,036] ERROR WorkerSourceTask{id=mongoDB-sourcetest-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask)
com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message}, caused by {java.net.SocketTimeoutException: Read timed out}}]
	at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
	at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
	at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:137)
	at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:94)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:276)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:176)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.execute(ChangeStreamIterableImpl.java:204)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.access$000(ChangeStreamIterableImpl.java:53)
	at com.mongodb.client.internal.ChangeStreamIterableImpl$1.cursor(ChangeStreamIterableImpl.java:129)
	at com.mongodb.client.internal.ChangeStreamIterableImpl$1.cursor(ChangeStreamIterableImpl.java:121)
	at com.mongodb.kafka.connect.source.MongoSourceTask.tryCreateCursor(MongoSourceTask.java:426)
	at com.mongodb.kafka.connect.source.MongoSourceTask.createCursor(MongoSourceTask.java:382)
	at com.mongodb.kafka.connect.source.MongoSourceTask.initializeCursorAndHeartbeatManager(MongoSourceTask.java:369)
	at com.mongodb.kafka.connect.source.MongoSourceTask.getNextDocument(MongoSourceTask.java:615)
	at com.mongodb.kafka.connect.source.MongoSourceTask.poll(MongoSourceTask.java:223)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:304)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:248)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-04-25 20:31:26,038] INFO Stopping MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 20:31:26,040] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2022-04-25 20:31:26,044] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 20:31:26,044] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 20:31:26,044] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2022-04-25 20:31:26,044] INFO App info kafka.producer for connector-producer-mongoDB-sourcetest-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 20:31:34,147] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:31:44,151] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:31:54,130] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:32:04,131] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:32:14,134] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:32:24,113] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:32:34,116] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:32:44,116] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:32:54,095] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:33:04,095] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:33:14,096] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:33:24,075] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:33:34,075] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:33:44,075] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:33:54,054] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:34:04,055] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:34:14,055] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:34:24,034] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:34:34,034] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:34:44,034] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:34:54,013] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:35:04,014] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:35:14,014] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:35:23,993] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:35:33,993] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:35:43,994] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:35:53,972] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:36:03,973] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:36:13,973] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:36:23,953] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:36:33,953] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:36:43,953] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:36:53,931] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:37:03,932] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:37:13,932] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:37:23,911] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:37:24,421] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:37:24,422] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:37:33,911] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:37:43,911] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:37:53,890] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:38:03,891] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:38:13,892] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:38:23,871] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:38:33,871] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:38:43,872] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:38:53,850] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:39:03,851] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:39:13,851] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:39:23,830] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:39:33,830] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:39:43,830] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:39:53,809] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:40:03,810] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:40:13,810] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:40:23,789] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:40:33,790] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:40:43,790] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:40:53,769] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:41:03,769] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:41:13,770] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:41:23,748] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:41:24,844] INFO [Producer clientId=producer-2] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:41:33,749] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:41:43,749] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:41:53,728] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:42:03,729] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:42:13,729] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:42:23,708] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:42:24,301] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:42:33,709] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:42:43,709] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:42:53,687] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:43:03,688] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:43:13,688] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:43:23,580] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:43:33,581] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:43:43,581] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:43:53,559] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:44:03,559] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:44:13,560] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:44:23,537] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:44:33,538] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:44:43,538] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:44:53,517] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:45:03,517] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:45:13,517] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:45:23,495] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:45:33,496] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:45:43,496] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:45:53,474] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:46:03,475] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:46:13,475] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:46:23,453] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:46:33,453] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:46:43,454] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:46:53,432] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:47:03,432] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:47:13,433] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:47:23,411] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:47:24,096] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:47:33,411] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:47:43,411] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:47:53,390] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:48:03,390] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:48:13,390] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:48:23,369] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:48:33,369] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:48:43,370] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:48:53,347] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:49:03,348] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:49:13,348] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:49:23,326] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:49:33,326] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:49:43,326] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:49:53,305] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:50:03,305] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:50:13,305] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:50:23,283] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:50:33,284] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:50:43,284] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:50:53,262] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:51:03,263] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:51:13,263] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:51:23,241] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:51:33,242] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:51:43,242] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:51:53,220] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:52:03,221] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:52:13,221] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:52:23,199] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:52:23,977] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 20:52:33,199] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:52:43,199] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:52:53,178] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:53:03,178] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:53:13,178] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:53:23,156] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:53:24,187] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 20:53:33,157] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:53:43,157] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:53:53,135] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:54:03,135] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:54:13,136] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:54:23,114] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:54:33,114] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:54:43,115] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:54:53,092] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:55:03,093] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:55:13,093] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:55:23,072] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:55:33,072] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:55:43,072] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:55:53,050] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:56:03,051] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:56:13,051] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:56:23,029] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:56:33,029] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:56:43,030] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:56:53,008] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:57:03,009] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:57:13,009] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:57:22,987] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:57:32,987] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:57:42,988] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:57:52,966] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:58:02,966] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:58:12,966] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:58:22,944] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:58:32,945] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:58:42,945] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:58:52,924] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:59:02,924] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:59:12,924] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:59:22,902] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:59:32,902] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:59:42,903] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 20:59:52,881] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:00:02,881] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:00:12,882] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:00:22,860] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:00:32,860] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:00:42,860] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:00:48,822] INFO Successfully processed removal of connector 'mongoDB-sourcetest' (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[2022-04-25 21:00:48,822] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,823] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:00:48,826] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling connector-only config update by stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,827] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:00:48,827] INFO Scheduled shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 21:00:48,833] INFO Completed shutdown for WorkerConnector{id=mongoDB-sourcetest} (org.apache.kafka.connect.runtime.WorkerConnector)
[2022-04-25 21:00:48,834] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,834] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,836] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=5, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,842] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=5, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,844] INFO Stopping task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:00:48,845] INFO Stopping connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:00:48,845] WARN Ignoring stop request for unowned connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:00:48,845] WARN Ignoring await stop request for non-present connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:00:48,846] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,846] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,846] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', leaderUrl='http://connect:8083/', offset=36, connectorIds=[], taskIds=[], revokedConnectorIds=[mongoDB-sourcetest], revokedTaskIds=[mongoDB-sourcetest-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,846] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 36 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,846] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,846] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,846] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,848] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=6, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,853] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=6, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:00:48,853] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', leaderUrl='http://connect:8083/', offset=36, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,854] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 36 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:00:48,854] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,553] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 21:01:35,558] INFO Opened connection [connectionId{localValue:125, serverValue:62}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 21:01:35,558] INFO Opened connection [connectionId{localValue:126, serverValue:63}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 21:01:35,559] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1967895, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=2, topologyVersion=TopologyVersion{processId=6267085160353cf642974f94, counter=7}, lastWriteDate=Mon Apr 25 21:01:33 GMT 2022, lastUpdateTimeNanos=21952345543877} (org.mongodb.driver.cluster)
[2022-04-25 21:01:35,560] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 21:01:35,564] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Connector mongoDB-sourcetest config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,565] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,565] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,566] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=7, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,573] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=7, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,573] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', leaderUrl='http://connect:8083/', offset=37, connectorIds=[mongoDB-sourcetest], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,575] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 37 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,576] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,576] INFO Creating connector mongoDB-sourcetest of type com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,576] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 21:01:35,577] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 21:01:35,577] INFO Instantiated connector mongoDB-sourcetest with version 1.6.1-dirty of type class com.mongodb.kafka.connect.MongoSourceConnector (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,578] INFO Finished creating connector mongoDB-sourcetest (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,578] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,582] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 21:01:35,582] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 21:01:35,591] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Tasks [mongoDB-sourcetest-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,592] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,592] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,592] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,594] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully joined group with generation Generation{generationId=8, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,599] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Successfully synced group in generation Generation{generationId=8, memberId='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2022-04-25 21:01:35,599] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6333fc77-3a7e-4c4e-954e-d4bdb2fc0440', leaderUrl='http://connect:8083/', offset=39, connectorIds=[mongoDB-sourcetest], taskIds=[mongoDB-sourcetest-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,599] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting connectors and tasks using config offset 39 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,600] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Starting task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,600] INFO Creating task mongoDB-sourcetest-0 (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,601] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2022-04-25 21:01:35,601] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 21:01:35,601] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.source.MongoSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2022-04-25 21:01:35,601] INFO Instantiated task mongoDB-sourcetest-0 with version 1.6.1-dirty of type com.mongodb.kafka.connect.source.MongoSourceTask (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,605] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 21:01:35,605] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,607] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig)
[2022-04-25 21:01:35,607] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,607] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongoDB-sourcetest-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,608] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2022-04-25 21:01:35,608] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mongoDB-sourcetest
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2022-04-25 21:01:35,608] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)
[2022-04-25 21:01:35,608] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka1:19091, kafka2:19092, kafka3:19093]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongoDB-sourcetest-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 21:01:35,613] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 21:01:35,614] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2022-04-25 21:01:35,614] INFO Kafka version: 7.1.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 21:01:35,614] INFO Kafka commitId: 947fac5beb61836d (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 21:01:35,614] INFO Kafka startTimeMs: 1650920495614 (org.apache.kafka.common.utils.AppInfoParser)
[2022-04-25 21:01:35,616] INFO Starting MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 21:01:35,616] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Cluster ID: rVFT0xMpRjSktFSh_qqZNw (org.apache.kafka.clients.Metadata)
[2022-04-25 21:01:35,616] INFO [Worker clientId=connect-1, groupId=connect-cluster-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2022-04-25 21:01:35,617] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 21:01:35,621] INFO Opened connection [connectionId{localValue:128, serverValue:65}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 21:01:35,621] INFO Opened connection [connectionId{localValue:127, serverValue:64}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 21:01:35,622] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2410103, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=2, topologyVersion=TopologyVersion{processId=6267085160353cf642974f94, counter=7}, lastWriteDate=Mon Apr 25 21:01:33 GMT 2022, lastUpdateTimeNanos=21952409584052} (org.mongodb.driver.cluster)
[2022-04-25 21:01:35,631] INFO Watching for collection changes on 'testdb.newcol2' (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 21:01:35,632] INFO New change stream cursor created without offset. (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 21:01:35,634] INFO Opened connection [connectionId{localValue:129, serverValue:66}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 21:01:35,637] INFO Started MongoDB source task (com.mongodb.kafka.connect.source.MongoSourceTask)
[2022-04-25 21:01:35,637] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:01:35,637] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:01:45,618] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:01:55,599] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:02:05,600] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:02:15,601] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:02:25,579] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:02:30,117] INFO Cluster created with settings {hosts=[host.docker.internal:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'} (org.mongodb.driver.cluster)
[2022-04-25 21:02:30,121] INFO Opened connection [connectionId{localValue:130, serverValue:67}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 21:02:30,121] INFO Monitor thread successfully connected to server with description ServerDescription{address=host.docker.internal:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=874573, setName='replset', canonicalAddress=host.docker.internal:27017, hosts=[host.docker.internal:27019, host.docker.internal:27018, host.docker.internal:27017], passives=[], arbiters=[], primary='host.docker.internal:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=2, topologyVersion=TopologyVersion{processId=6267085160353cf642974f94, counter=7}, lastWriteDate=Mon Apr 25 21:02:23 GMT 2022, lastUpdateTimeNanos=22006953383248} (org.mongodb.driver.cluster)
[2022-04-25 21:02:30,121] INFO Opened connection [connectionId{localValue:131, serverValue:68}] to host.docker.internal:27017 (org.mongodb.driver.connection)
[2022-04-25 21:02:30,123] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2022-04-25 21:02:35,579] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:02:45,580] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:02:55,558] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:03:05,559] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:03:15,560] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:03:25,538] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:03:35,539] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:03:45,539] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:03:55,512] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:04:05,512] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:04:15,513] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:04:25,495] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:04:35,495] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:04:45,496] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:04:55,474] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:05:05,474] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:05:15,475] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:05:25,453] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:05:35,454] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:05:45,454] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:05:55,433] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:06:05,433] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:06:15,434] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:06:25,412] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:06:35,413] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:06:45,414] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:06:55,393] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:07:05,393] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:07:15,394] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:07:25,374] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:07:35,377] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:07:45,377] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:07:55,356] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:08:05,357] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:08:15,357] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:08:25,336] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:08:35,337] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:08:45,338] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:08:55,317] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:09:05,317] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:09:15,317] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:09:25,296] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:09:35,297] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:09:45,298] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:09:54,845] INFO [Producer clientId=producer-2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:09:55,276] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:10:05,277] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:10:15,278] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:10:25,257] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:10:35,260] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:10:35,616] INFO [Producer clientId=connector-producer-mongoDB-sourcetest-0] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:10:45,260] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:10:48,707] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:10:48,707] INFO [AdminClient clientId=adminclient-8] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:10:54,837] INFO [Producer clientId=producer-3] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:10:55,238] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:11:05,239] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:11:15,240] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:11:25,218] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:11:35,219] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:11:45,220] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:11:55,199] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:12:05,201] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:12:15,202] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:12:25,182] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:12:35,184] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:12:45,184] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:12:55,164] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:13:05,164] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:13:15,165] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:13:25,144] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:13:35,148] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:13:45,150] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:13:55,129] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:14:05,131] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:14:15,131] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:14:25,112] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:14:35,112] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:14:45,113] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:14:55,091] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:15:05,093] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:15:15,095] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:15:24,851] INFO [Producer clientId=producer-2] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:15:25,074] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:15:35,074] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:15:45,075] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:15:48,587] INFO [AdminClient clientId=adminclient-8] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:15:55,054] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:16:05,054] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:16:15,055] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:16:24,799] INFO [Producer clientId=producer-1] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2022-04-25 21:16:25,033] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:16:35,034] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:16:45,035] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:16:55,015] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:17:05,016] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:17:15,017] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:17:24,995] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:17:34,996] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:17:44,996] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:17:54,975] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:18:04,976] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:18:14,977] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:18:24,955] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:18:34,956] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:18:44,958] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:18:54,936] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:19:04,937] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2022-04-25 21:19:14,938] INFO WorkerSourceTask{id=mongoDB-sourcetest-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask)
